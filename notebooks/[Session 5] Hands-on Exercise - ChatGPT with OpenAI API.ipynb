{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT is powered by `gpt-3.5-turbo` and `gpt-4`, OpenAI's most advanced models.\n",
    "\n",
    "You can build your own applications with `gpt-3.5-turbo` or `gpt-4` using the OpenAI API.\n",
    "\n",
    "Chat models take a series of messages as input, and return an AI-written message as output.\n",
    "\n",
    "This guide illustrates the chat format with a few example API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the OpenAI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install tiktoken\n",
    "# !pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "# %pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OpenAI Python library for calling the OpenAI API\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the API key\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# import os\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. An example chat API call\n",
    "\n",
    "A chat API call has two required inputs:\n",
    "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-3.5-turbo-0613`, `gpt-3.5-turbo-16k-0613`)\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Messages can also contain an optional `name` field, which give the messenger a name. E.g., `example-user`, `Alice`, `BlackbeardBot`. Names may not contain spaces.\n",
    "\n",
    "As of June 2023, you can also optionally submit a list of `functions` that tell GPT whether it can generate JSON to feed into a function. For details, see the [documentation](https://platform.openai.com/docs/guides/gpt/function-calling), [API reference](https://platform.openai.com/docs/api-reference/chat), or the Cookbook guide [How to call functions with chat models](How_to_call_functions_with_chat_models.ipynb).\n",
    "\n",
    "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7bB8ESoBGX9bhZFAVcE3fJQCLmRJt at 0x7f4a4136f9f0> JSON: {\n",
       "  \"id\": \"chatcmpl-7bB8ESoBGX9bhZFAVcE3fJQCLmRJt\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1689095282,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Orange who?\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 35,\n",
       "    \"completion_tokens\": 3,\n",
       "    \"total_tokens\": 38\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example OpenAI Python library request\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orange who?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
    "\n",
    "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me matey! Let me tell ye a tale of asynchronous programming, in the style of the fearsome pirate Blackbeard!\n",
      "\n",
      "Picture this, me hearties. In the vast ocean of programming, there be times when ye need to perform multiple tasks at once. But wait, ye can't just be waitin' around for each task to finish before movin' on to the next one, now can ye? That be where asynchronous programming comes in!\n",
      "\n",
      "Ye see, in the world of asynchronous programming, ye can be startin' a task and then movin' on to another one without waitin' for the first one to be done. It be like havin' a crew of pirates workin' on different tasks at the same time, each one doin' their own thing.\n",
      "\n",
      "Now, ye may be wonderin', how does this sorcery work? Well, me matey, it be all about callbacks and promises. When ye start a task, ye give it a callback function, which be like a message in a bottle. This callback be tellin' ye what to do once the task be finished. So ye can be movin' on to the next task while ye be waitin' for the first one to be done.\n",
      "\n",
      "But wait, there be more! Promises be like treasure maps, showin' ye the way to the booty. Ye can be creatin' a promise for a task, and then ye can be doin' other things while ye be waitin' for that promise to be fulfilled. Once the promise be fulfilled, ye can be doin' somethin' with the result, like findin' the hidden treasure.\n",
      "\n",
      "So, me hearties, asynchronous programming be like a pirate's life on the high seas. Ye can be multitaskin' like a true buccaneer, startin' tasks and movin' on to the next one without waitin' around. It be a powerful tool in the programmer's arsenal, allowin' ye to be more efficient and productive.\n",
      "\n",
      "Now, go forth, me mateys, and embrace the ways of asynchronous programming like the fearsome pirate ye be! Yo ho ho!\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, me hearties! Gather 'round and listen up, for I be tellin' ye about the mysterious art of asynchronous programming, in the style of the fearsome pirate Blackbeard!\n",
      "\n",
      "Now, ye see, in the world of programming, there be times when we need to perform tasks that take a mighty long time to complete. These tasks might involve fetchin' data from the depths of the internet, or performin' complex calculations that would make even Davy Jones scratch his head.\n",
      "\n",
      "In the olden days, we pirates used to wait patiently for each task to finish afore movin' on to the next one. But that be a waste of precious time, me hearties! We be pirates, always lookin' for ways to be more efficient and plunder more booty!\n",
      "\n",
      "That be where asynchronous programming comes in, me mateys. It be a way to tackle multiple tasks at once, without waitin' for each one to finish afore movin' on. It be like havin' a crew of scallywags workin' on different tasks simultaneously, while ye be overseein' the whole operation.\n",
      "\n",
      "Ye see, in asynchronous programming, we be breakin' down our tasks into smaller chunks called \"coroutines.\" Each coroutine be like a separate pirate, workin' on its own task. When a coroutine be startin' its work, it don't wait for the task to finish afore movin' on to the next one. Instead, it be movin' on to the next task, lettin' the first one continue in the background.\n",
      "\n",
      "Now, ye might be wonderin', how be these coroutines coordinatin' their work? Well, me hearties, they be sendin' messages to each other, like messages in a bottle floatin' in the sea. When a coroutine be needin' somethin' from another coroutine, it sends a message and waits for a response. In the meantime, it be free to work on other tasks or rest its weary bones.\n",
      "\n",
      "Once a coroutine receives a response, it be pickin' up where it left off, continuin' its work with the new information. This be happenin' in a non-linear fashion, like a ship sailin' through a storm, changin' course whenever it needs to.\n",
      "\n",
      "Asynchronous programming be a powerful tool, me mateys. It be allowin' us to make our programs faster and more responsive. It be like havin' a fleet of ships, each sailin' its own course, yet all arrivin' at the same destination.\n",
      "\n",
      "So, me hearties, next time ye be writin' code, remember the ways of asynchronous programming. Be like Blackbeard, masterin' the art of multitaskin' and conquerin' the high seas of programming!\n"
     ]
    }
   ],
   "source": [
    "# example without a system message\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Tips for instructing gpt-3.5-turbo-0301\n",
    "\n",
    "Best practices for instructing models may change from model version to model version. The advice that follows applies to `gpt-3.5-turbo-0301` and may not apply to future models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### System messages\n",
    "\n",
    "The system message can be used to prime the assistant with different personalities or behaviors.\n",
    "\n",
    "Be aware that `gpt-3.5-turbo-0301` does not generally pay as much attention to the system message as `gpt-4-0314` or `gpt-3.5-turbo-0613`. Therefore, for `gpt-3.5-turbo-0301`, we recommend placing important instructions in the user message instead. Some developers have found success in continually moving the system message near the end of the conversation to keep the model's attention from drifting away as conversations get longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Fractions are a way to represent parts of a whole. They are made up of two numbers: a numerator and a denominator. The numerator tells you how many parts you have, and the denominator tells you how many equal parts make up the whole.\n",
      "\n",
      "Let's take an example to understand this better. Imagine you have a pizza that is divided into 8 equal slices. If you eat 3 slices, you can represent that as the fraction 3/8. Here, the numerator is 3 because you ate 3 slices, and the denominator is 8 because the whole pizza is divided into 8 slices.\n",
      "\n",
      "Fractions can also be used to represent numbers less than 1. For example, if you eat half of a pizza, you can write it as 1/2. Here, the numerator is 1 because you ate one slice, and the denominator is 2 because the whole pizza is divided into 2 equal parts.\n",
      "\n",
      "Now, let's talk about equivalent fractions. Equivalent fractions are different fractions that represent the same amount. For example, 1/2 and 2/4 are equivalent fractions because they both represent half of something. To find equivalent fractions, you can multiply or divide both the numerator and denominator by the same number.\n",
      "\n",
      "Now, here's a question for you: If you have a pizza divided into 12 equal slices and you eat 4 slices, what fraction of the pizza did you eat?\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to explain concepts in great depth\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractions represent parts of a whole.\n"
     ]
    }
   ],
   "source": [
    "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have enough time to complete the entire project for the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can try setting the `name` field of `system` messages to `example_user` and `example_assistant`.\n",
    "\n",
    "Transforming the few-shot example above, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sudden change in direction means we don't have enough time to complete the entire project for the client.\n"
     ]
    }
   ],
   "source": [
    "# The business jargon translation example, but with example names for the example messages\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not every attempt at engineering conversations will succeed at first.\n",
    "\n",
    "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
    "\n",
    "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
    "\n",
    "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Counting tokens\n",
    "\n",
    "When you submit your request, the API transforms the messages into a sequence of tokens.\n",
    "\n",
    "The number of tokens used affects:\n",
    "- the cost of the request\n",
    "- the time it takes to generate the response\n",
    "- when the reply gets cut off from hitting the maximum token limit (4,096 for `gpt-3.5-turbo` or 8,192 for `gpt-4`)\n",
    "\n",
    "You can use the following function to count the number of tokens that a list of messages will use.\n",
    "\n",
    "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee. \n",
    "\n",
    "In particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below.\n",
    "\n",
    "Read more about counting tokens in [How to count tokens with tiktoken](How_to_count_tokens_with_tiktoken.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-3.5-turbo-0301\n",
      "127 prompt tokens counted by num_tokens_from_messages().\n",
      "127 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-3.5-turbo\n",
      "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0314\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4-0613\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n",
      "gpt-4\n",
      "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
      "129 prompt tokens counted by num_tokens_from_messages().\n",
      "129 prompt tokens counted by the OpenAI API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's verify the function above matches the OpenAI API response\n",
    "\n",
    "import openai\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",\n",
    "    ]:\n",
    "    print(model)\n",
    "    # example token count from the function defined above\n",
    "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
    "    # example token count from the OpenAI API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,  # we're only counting input tokens here, so let's not waste tokens on the output\n",
    "    )\n",
    "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6. Handling Rate Limits\n",
    "\n",
    "When you call the OpenAI API repeatedly, you may encounter error messages that say `429: 'Too Many Requests'` or `RateLimitError`. These error messages come from exceeding the API's rate limits.\n",
    "\n",
    "This guide shares tips for avoiding and handling rate limit errors.\n",
    "\n",
    "To see an example script for throttling parallel requests to avoid rate limit errors, see [api_request_parallel_processor.py](api_request_parallel_processor.py).\n",
    "\n",
    "## Why rate limits exist\n",
    "\n",
    "Rate limits are a common practice for APIs, and they're put in place for a few different reasons.\n",
    "\n",
    "- First, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\n",
    "- Second, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns.\n",
    "- Lastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n",
    "\n",
    "Although hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users.\n",
    "\n",
    "## Default rate limits\n",
    "\n",
    "As of Jan 2023, the default rate limits are:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Text Completion &amp; Embedding endpoints</th>\n",
    "    <th>Code &amp; Edit endpoints</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Free trial users</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pay-as-you-go users (in your first 48 hours)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>60 requests / minute</li>\n",
    "            <li>250,000 davinci tokens / minute (and proportionally more for cheaper models)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pay-as-you-go users (after your first 48 hours)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>3,000 requests / minute</li>\n",
    "            <li>250,000 davinci tokens / minute (and proportionally more for cheaper models)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "For reference, 1,000 tokens is roughly a page of text.\n",
    "\n",
    "### Other rate limit resources\n",
    "\n",
    "Read more about OpenAI's rate limits in these other resources:\n",
    "\n",
    "- [Guide: Rate limits](https://beta.openai.com/docs/guides/rate-limits/overview)\n",
    "- [Help Center: Is API usage subject to any rate limits?](https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits)\n",
    "- [Help Center: How can I solve 429: 'Too Many Requests' errors?](https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors)\n",
    "\n",
    "### Requesting a rate limit increase\n",
    "\n",
    "If you'd like your organization's rate limit increased, please fill out the following form:\n",
    "\n",
    "- [OpenAI Rate Limit Increase Request form](https://forms.gle/56ZrwXXoxAN1yt6i9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Example rate limit error\n",
    "\n",
    "A rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:\n",
    "\n",
    "```\n",
    "RateLimitError: Rate limit reached for default-codex in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. Contact support@openai.com if you continue to have issues or if you’d like to request an increase.\n",
    "```\n",
    "\n",
    "Below is example code for triggering a rate limit error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import openai  # for making OpenAI API requests\n",
    "\n",
    "# # request a bunch of completions in a loop\n",
    "# for _ in range(100):\n",
    "#     openai.Completion.create(\n",
    "#         model=\"code-cushman-001\",\n",
    "#         prompt=\"def magic_function():\\n\\t\",\n",
    "#         max_tokens=10,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### How to avoid rate limit errors\n",
    "\n",
    "#### Retrying with exponential backoff\n",
    "\n",
    "One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\n",
    "\n",
    "This approach has many benefits:\n",
    "\n",
    "- Automatic retries means you can recover from rate limit errors without crashes or missing data\n",
    "- Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\n",
    "- Adding random jitter to the delay helps retries from all hitting at the same time\n",
    "\n",
    "Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.\n",
    "\n",
    "Below are a few example solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Using the Tenacity library\n",
    "\n",
    "[Tenacity](https://tenacity.readthedocs.io/en/latest/) is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\n",
    "\n",
    "To add exponential backoff to your requests, you can use the `tenacity.retry` [decorator](https://peps.python.org/pep-0318/). The following example uses the `tenacity.wait_random_exponential` function to add random exponential backoff to a request.\n",
    "\n",
    "Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7bBBtxM77T8hjO0725exQwbudifqL at 0x7f4a78f005e0> JSON: {\n",
       "  \"id\": \"cmpl-7bBBtxM77T8hjO0725exQwbudifqL\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1689095509,\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \" a little girl went to bed, and slept until morning.\\n\\nIt was\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 16,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai  # for OpenAI API calls\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7. How to stream completions\n",
    "\n",
    "By default, when you request a completion from the OpenAI, the entire completion is generated before being sent back in a single response.\n",
    "\n",
    "If you're generating long completions, waiting for the response can take many seconds.\n",
    "\n",
    "To get responses sooner, you can 'stream' the completion as it's being generated. This allows you to start printing or processing the beginning of the completion before the full completion is finished.\n",
    "\n",
    "To stream completions, set `stream=True` when calling the chat completions or completions endpoints. This will return an object that streams back the response as [data-only server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). Extract chunks from the `delta` field rather than the `message` field.\n",
    "\n",
    "### Downsides\n",
    "\n",
    "Note that using `stream=True` in a production application makes it more difficult to moderate the content of the completions, as partial completions may be more difficult to evaluate. which has implications for [approved usage](https://beta.openai.com/docs/usage-guidelines).\n",
    "\n",
    "Another small drawback of streaming responses is that the response no longer includes the `usage` field to tell you how many tokens were consumed. After receiving and combining all of the responses, you can calculate this yourself using [`tiktoken`](How_to_count_tokens_with_tiktoken.ipynb).\n",
    "\n",
    "### Example code\n",
    "\n",
    "Below is shown:\n",
    "1. What a typical chat completion response looks like\n",
    "2. What a streaming chat completion response looks like\n",
    "3. How much time is saved by streaming a chat completion\n",
    "4. How to stream non-chat completions (used by older models like `text-davinci-003`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import openai  # for OpenAI API calls\n",
    "import time  # for measuring time duration of API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1. What a typical chat completion response looks like\n",
    "\n",
    "With a typical ChatCompletions API call, the response is first computed and then returned all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response received 7.26 seconds after request\n",
      "Full response received:\n",
      "{\n",
      "  \"id\": \"chatcmpl-7bBByF8MYndDtUWtRFWjDhlF0lbWD\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1689095514,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"completion_tokens\": 299,\n",
      "    \"total_tokens\": 335\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example of an OpenAI ChatCompletion request\n",
    "# https://platform.openai.com/docs/guides/chat\n",
    "\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a ChatCompletion request to count to 100\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# calculate the time it took to receive the response\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
    "print(f\"Full response received:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The reply can be extracted with `response['choices'][0]['message']`.\n",
    "\n",
    "The content of the reply can be extracted with `response['choices'][0]['message']['content']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted reply: \n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\"\n",
      "}\n",
      "Extracted content: \n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n"
     ]
    }
   ],
   "source": [
    "reply = response['choices'][0]['message']\n",
    "print(f\"Extracted reply: \\n{reply}\")\n",
    "\n",
    "reply_content = response['choices'][0]['message']['content']\n",
    "print(f\"Extracted content: \\n{reply_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2. How to stream a chat completion\n",
    "\n",
    "With a streaming API call, the response is sent back incrementally in chunks via an [event stream](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format). In Python, you can iterate over these events with a `for` loop.\n",
    "\n",
    "Let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7bBC88ghfuNopoUyUz7CAScIupcbu\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1689095524,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7bBC88ghfuNopoUyUz7CAScIupcbu\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1689095524,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {\n",
      "        \"content\": \"2\"\n",
      "      },\n",
      "      \"finish_reason\": null\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"id\": \"chatcmpl-7bBC88ghfuNopoUyUz7CAScIupcbu\",\n",
      "  \"object\": \"chat.completion.chunk\",\n",
      "  \"created\": 1689095524,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"delta\": {},\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example of an OpenAI ChatCompletion request with stream=True\n",
    "# https://platform.openai.com/docs/guides/chat\n",
    "\n",
    "# a ChatCompletion request\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"What's 1+1? Answer in one word.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see above, streaming responses have a `delta` field rather than a `message` field. `delta` can hold things like:\n",
    "- a role token (e.g., `{\"role\": \"assistant\"}`)\n",
    "- a content token (e.g., `{\"content\": \"\\n\\n\"}`)\n",
    "- nothing (e.g., `{}`), when the stream is over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3. How much time is saved by streaming a chat completion\n",
    "\n",
    "Now let's ask `gpt-3.5-turbo` to count to 100 again, and see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message received 0.39 seconds after request: {\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"\"\n",
      "}\n",
      "Message received 0.39 seconds after request: {\n",
      "  \"content\": \"1\"\n",
      "}\n",
      "Message received 0.44 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.48 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.50 seconds after request: {\n",
      "  \"content\": \"2\"\n",
      "}\n",
      "Message received 0.53 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.55 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.57 seconds after request: {\n",
      "  \"content\": \"3\"\n",
      "}\n",
      "Message received 0.60 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.62 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.64 seconds after request: {\n",
      "  \"content\": \"4\"\n",
      "}\n",
      "Message received 0.66 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.68 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.70 seconds after request: {\n",
      "  \"content\": \"5\"\n",
      "}\n",
      "Message received 0.72 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.74 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.76 seconds after request: {\n",
      "  \"content\": \"6\"\n",
      "}\n",
      "Message received 0.79 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.80 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.82 seconds after request: {\n",
      "  \"content\": \"7\"\n",
      "}\n",
      "Message received 0.84 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.87 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.90 seconds after request: {\n",
      "  \"content\": \"8\"\n",
      "}\n",
      "Message received 0.92 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 0.94 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 0.96 seconds after request: {\n",
      "  \"content\": \"9\"\n",
      "}\n",
      "Message received 0.98 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.00 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.03 seconds after request: {\n",
      "  \"content\": \"10\"\n",
      "}\n",
      "Message received 1.06 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.08 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.11 seconds after request: {\n",
      "  \"content\": \"11\"\n",
      "}\n",
      "Message received 1.13 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.15 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.17 seconds after request: {\n",
      "  \"content\": \"12\"\n",
      "}\n",
      "Message received 1.19 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.22 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.25 seconds after request: {\n",
      "  \"content\": \"13\"\n",
      "}\n",
      "Message received 1.27 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.29 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.31 seconds after request: {\n",
      "  \"content\": \"14\"\n",
      "}\n",
      "Message received 1.35 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.36 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.40 seconds after request: {\n",
      "  \"content\": \"15\"\n",
      "}\n",
      "Message received 1.42 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.43 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.45 seconds after request: {\n",
      "  \"content\": \"16\"\n",
      "}\n",
      "Message received 1.49 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.50 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.53 seconds after request: {\n",
      "  \"content\": \"17\"\n",
      "}\n",
      "Message received 1.54 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.56 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \"18\"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \"19\"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \"20\"\n",
      "}\n",
      "Message received 1.71 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.72 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.74 seconds after request: {\n",
      "  \"content\": \"21\"\n",
      "}\n",
      "Message received 1.76 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.77 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.80 seconds after request: {\n",
      "  \"content\": \"22\"\n",
      "}\n",
      "Message received 1.81 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.82 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.84 seconds after request: {\n",
      "  \"content\": \"23\"\n",
      "}\n",
      "Message received 1.86 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.88 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.92 seconds after request: {\n",
      "  \"content\": \"24\"\n",
      "}\n",
      "Message received 1.92 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.93 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 1.95 seconds after request: {\n",
      "  \"content\": \"25\"\n",
      "}\n",
      "Message received 1.96 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 1.99 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.00 seconds after request: {\n",
      "  \"content\": \"26\"\n",
      "}\n",
      "Message received 2.02 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.04 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.06 seconds after request: {\n",
      "  \"content\": \"27\"\n",
      "}\n",
      "Message received 2.09 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.09 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.12 seconds after request: {\n",
      "  \"content\": \"28\"\n",
      "}\n",
      "Message received 2.13 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.15 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.16 seconds after request: {\n",
      "  \"content\": \"29\"\n",
      "}\n",
      "Message received 2.18 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.20 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.22 seconds after request: {\n",
      "  \"content\": \"30\"\n",
      "}\n",
      "Message received 2.23 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.25 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.27 seconds after request: {\n",
      "  \"content\": \"31\"\n",
      "}\n",
      "Message received 2.29 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.30 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.32 seconds after request: {\n",
      "  \"content\": \"32\"\n",
      "}\n",
      "Message received 2.34 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.36 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.37 seconds after request: {\n",
      "  \"content\": \"33\"\n",
      "}\n",
      "Message received 2.39 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.41 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.43 seconds after request: {\n",
      "  \"content\": \"34\"\n",
      "}\n",
      "Message received 2.45 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.47 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.48 seconds after request: {\n",
      "  \"content\": \"35\"\n",
      "}\n",
      "Message received 2.50 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.52 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.54 seconds after request: {\n",
      "  \"content\": \"36\"\n",
      "}\n",
      "Message received 2.56 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.58 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.59 seconds after request: {\n",
      "  \"content\": \"37\"\n",
      "}\n",
      "Message received 2.61 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.62 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.64 seconds after request: {\n",
      "  \"content\": \"38\"\n",
      "}\n",
      "Message received 2.65 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.69 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.71 seconds after request: {\n",
      "  \"content\": \"39\"\n",
      "}\n",
      "Message received 2.72 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.75 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.77 seconds after request: {\n",
      "  \"content\": \"40\"\n",
      "}\n",
      "Message received 2.79 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.81 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.83 seconds after request: {\n",
      "  \"content\": \"41\"\n",
      "}\n",
      "Message received 2.85 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.86 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 2.89 seconds after request: {\n",
      "  \"content\": \"42\"\n",
      "}\n",
      "Message received 2.90 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 2.91 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.02 seconds after request: {\n",
      "  \"content\": \"43\"\n",
      "}\n",
      "Message received 3.03 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.03 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.03 seconds after request: {\n",
      "  \"content\": \"44\"\n",
      "}\n",
      "Message received 3.04 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.04 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.04 seconds after request: {\n",
      "  \"content\": \"45\"\n",
      "}\n",
      "Message received 3.04 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.06 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.08 seconds after request: {\n",
      "  \"content\": \"46\"\n",
      "}\n",
      "Message received 3.10 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.11 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.12 seconds after request: {\n",
      "  \"content\": \"47\"\n",
      "}\n",
      "Message received 3.15 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.15 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.17 seconds after request: {\n",
      "  \"content\": \"48\"\n",
      "}\n",
      "Message received 3.18 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.20 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.21 seconds after request: {\n",
      "  \"content\": \"49\"\n",
      "}\n",
      "Message received 3.23 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.24 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.25 seconds after request: {\n",
      "  \"content\": \"50\"\n",
      "}\n",
      "Message received 3.27 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.28 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.30 seconds after request: {\n",
      "  \"content\": \"51\"\n",
      "}\n",
      "Message received 3.31 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.32 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.34 seconds after request: {\n",
      "  \"content\": \"52\"\n",
      "}\n",
      "Message received 3.36 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.37 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.38 seconds after request: {\n",
      "  \"content\": \"53\"\n",
      "}\n",
      "Message received 3.40 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.42 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.43 seconds after request: {\n",
      "  \"content\": \"54\"\n",
      "}\n",
      "Message received 3.46 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.48 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.50 seconds after request: {\n",
      "  \"content\": \"55\"\n",
      "}\n",
      "Message received 3.51 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.53 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.55 seconds after request: {\n",
      "  \"content\": \"56\"\n",
      "}\n",
      "Message received 3.56 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.58 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.60 seconds after request: {\n",
      "  \"content\": \"57\"\n",
      "}\n",
      "Message received 3.61 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.63 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.64 seconds after request: {\n",
      "  \"content\": \"58\"\n",
      "}\n",
      "Message received 3.66 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.68 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.70 seconds after request: {\n",
      "  \"content\": \"59\"\n",
      "}\n",
      "Message received 3.71 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.73 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.75 seconds after request: {\n",
      "  \"content\": \"60\"\n",
      "}\n",
      "Message received 3.76 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.91 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.91 seconds after request: {\n",
      "  \"content\": \"61\"\n",
      "}\n",
      "Message received 3.91 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.91 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.91 seconds after request: {\n",
      "  \"content\": \"62\"\n",
      "}\n",
      "Message received 3.92 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.92 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.92 seconds after request: {\n",
      "  \"content\": \"63\"\n",
      "}\n",
      "Message received 3.92 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.92 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.93 seconds after request: {\n",
      "  \"content\": \"64\"\n",
      "}\n",
      "Message received 3.93 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.94 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 3.96 seconds after request: {\n",
      "  \"content\": \"65\"\n",
      "}\n",
      "Message received 3.98 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 3.99 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.00 seconds after request: {\n",
      "  \"content\": \"66\"\n",
      "}\n",
      "Message received 4.03 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.03 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.05 seconds after request: {\n",
      "  \"content\": \"67\"\n",
      "}\n",
      "Message received 4.06 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.08 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.09 seconds after request: {\n",
      "  \"content\": \"68\"\n",
      "}\n",
      "Message received 4.14 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.14 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.14 seconds after request: {\n",
      "  \"content\": \"69\"\n",
      "}\n",
      "Message received 4.15 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.16 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.19 seconds after request: {\n",
      "  \"content\": \"70\"\n",
      "}\n",
      "Message received 4.19 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.22 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.22 seconds after request: {\n",
      "  \"content\": \"71\"\n",
      "}\n",
      "Message received 4.23 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.25 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.27 seconds after request: {\n",
      "  \"content\": \"72\"\n",
      "}\n",
      "Message received 4.28 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.29 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.30 seconds after request: {\n",
      "  \"content\": \"73\"\n",
      "}\n",
      "Message received 4.32 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.34 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.35 seconds after request: {\n",
      "  \"content\": \"74\"\n",
      "}\n",
      "Message received 4.38 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.39 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.40 seconds after request: {\n",
      "  \"content\": \"75\"\n",
      "}\n",
      "Message received 4.41 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.42 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.44 seconds after request: {\n",
      "  \"content\": \"76\"\n",
      "}\n",
      "Message received 4.45 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.47 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.48 seconds after request: {\n",
      "  \"content\": \"77\"\n",
      "}\n",
      "Message received 4.50 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.51 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.53 seconds after request: {\n",
      "  \"content\": \"78\"\n",
      "}\n",
      "Message received 4.54 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.56 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.58 seconds after request: {\n",
      "  \"content\": \"79\"\n",
      "}\n",
      "Message received 4.59 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.61 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.62 seconds after request: {\n",
      "  \"content\": \"80\"\n",
      "}\n",
      "Message received 4.63 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.64 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.68 seconds after request: {\n",
      "  \"content\": \"81\"\n",
      "}\n",
      "Message received 4.70 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.72 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.73 seconds after request: {\n",
      "  \"content\": \"82\"\n",
      "}\n",
      "Message received 4.75 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.77 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.78 seconds after request: {\n",
      "  \"content\": \"83\"\n",
      "}\n",
      "Message received 4.79 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.81 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.83 seconds after request: {\n",
      "  \"content\": \"84\"\n",
      "}\n",
      "Message received 4.84 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.85 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.87 seconds after request: {\n",
      "  \"content\": \"85\"\n",
      "}\n",
      "Message received 4.90 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.90 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.91 seconds after request: {\n",
      "  \"content\": \"86\"\n",
      "}\n",
      "Message received 4.92 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.94 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.96 seconds after request: {\n",
      "  \"content\": \"87\"\n",
      "}\n",
      "Message received 4.97 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 4.99 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 4.99 seconds after request: {\n",
      "  \"content\": \"88\"\n",
      "}\n",
      "Message received 5.01 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.02 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.06 seconds after request: {\n",
      "  \"content\": \"89\"\n",
      "}\n",
      "Message received 5.06 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.07 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.08 seconds after request: {\n",
      "  \"content\": \"90\"\n",
      "}\n",
      "Message received 5.10 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.11 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.13 seconds after request: {\n",
      "  \"content\": \"91\"\n",
      "}\n",
      "Message received 5.16 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.16 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.18 seconds after request: {\n",
      "  \"content\": \"92\"\n",
      "}\n",
      "Message received 5.19 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.20 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.21 seconds after request: {\n",
      "  \"content\": \"93\"\n",
      "}\n",
      "Message received 5.23 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.25 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.26 seconds after request: {\n",
      "  \"content\": \"94\"\n",
      "}\n",
      "Message received 5.27 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.30 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.33 seconds after request: {\n",
      "  \"content\": \"95\"\n",
      "}\n",
      "Message received 5.35 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.36 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.38 seconds after request: {\n",
      "  \"content\": \"96\"\n",
      "}\n",
      "Message received 5.40 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.41 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.43 seconds after request: {\n",
      "  \"content\": \"97\"\n",
      "}\n",
      "Message received 5.45 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.47 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.48 seconds after request: {\n",
      "  \"content\": \"98\"\n",
      "}\n",
      "Message received 5.50 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.53 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.53 seconds after request: {\n",
      "  \"content\": \"99\"\n",
      "}\n",
      "Message received 5.55 seconds after request: {\n",
      "  \"content\": \",\"\n",
      "}\n",
      "Message received 5.64 seconds after request: {\n",
      "  \"content\": \" \"\n",
      "}\n",
      "Message received 5.64 seconds after request: {\n",
      "  \"content\": \"100\"\n",
      "}\n",
      "Message received 5.65 seconds after request: {\n",
      "  \"content\": \".\"\n",
      "}\n",
      "Message received 5.65 seconds after request: {}\n",
      "Full response received 5.65 seconds after request\n",
      "Full conversation received: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100.\n"
     ]
    }
   ],
   "source": [
    "# Example of an OpenAI ChatCompletion request with stream=True\n",
    "# https://platform.openai.com/docs/guides/chat\n",
    "\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a ChatCompletion request to count to 100\n",
    "response = openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Count to 100, with a comma between each number and no newlines. E.g., 1, 2, 3, ...'}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    stream=True  # again, we set stream=True\n",
    ")\n",
    "\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
    "    collected_messages.append(chunk_message)  # save the message\n",
    "    print(f\"Message received {chunk_time:.2f} seconds after request: {chunk_message}\")  # print the delay and text\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "full_reply_content = ''.join([m.get('content', '') for m in collected_messages])\n",
    "print(f\"Full conversation received: {full_reply_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Time comparison\n",
    "\n",
    "In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.\n",
    "\n",
    "However, with the streaming request, we received the first token after 0.1 seconds, and subsequent tokens every ~0.01-0.02 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4. How to stream non-chat completions (used by older models like `text-davinci-003`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### A typical completion request\n",
    "\n",
    "With a typical Completions API call, the text is first computed and then returned all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response received 4.03 seconds after request\n",
      "Full text received: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n"
     ]
    }
   ],
   "source": [
    "# Example of an OpenAI Completion request\n",
    "# https://beta.openai.com/docs/api-reference/completions/create\n",
    "\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a Completion request to count to 100\n",
    "response = openai.Completion.create(\n",
    "    model='text-davinci-002',\n",
    "    prompt='1,2,3,',\n",
    "    max_tokens=193,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# calculate the time it took to receive the response\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "# extract the text from the response\n",
    "completion_text = response['choices'][0]['text']\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {response_time:.2f} seconds after request\")\n",
    "print(f\"Full text received: {completion_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### A streaming completion request\n",
    "\n",
    "With a streaming Completions API call, the text is sent back via a series of events. In Python, you can iterate over these events with a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text received: 4 (0.27 seconds after request)\n",
      "Text received: , (0.28 seconds after request)\n",
      "Text received: 5 (0.30 seconds after request)\n",
      "Text received: , (0.31 seconds after request)\n",
      "Text received: 6 (0.34 seconds after request)\n",
      "Text received: , (0.37 seconds after request)\n",
      "Text received: 7 (0.40 seconds after request)\n",
      "Text received: , (0.42 seconds after request)\n",
      "Text received: 8 (0.45 seconds after request)\n",
      "Text received: , (0.46 seconds after request)\n",
      "Text received: 9 (0.48 seconds after request)\n",
      "Text received: , (0.49 seconds after request)\n",
      "Text received: 10 (0.51 seconds after request)\n",
      "Text received: , (0.52 seconds after request)\n",
      "Text received: 11 (0.54 seconds after request)\n",
      "Text received: , (0.55 seconds after request)\n",
      "Text received: 12 (0.57 seconds after request)\n",
      "Text received: , (0.59 seconds after request)\n",
      "Text received: 13 (0.61 seconds after request)\n",
      "Text received: , (0.63 seconds after request)\n",
      "Text received: 14 (0.64 seconds after request)\n",
      "Text received: , (0.66 seconds after request)\n",
      "Text received: 15 (0.68 seconds after request)\n",
      "Text received: , (0.70 seconds after request)\n",
      "Text received: 16 (0.73 seconds after request)\n",
      "Text received: , (0.77 seconds after request)\n",
      "Text received: 17 (0.78 seconds after request)\n",
      "Text received: , (0.82 seconds after request)\n",
      "Text received: 18 (0.85 seconds after request)\n",
      "Text received: , (0.88 seconds after request)\n",
      "Text received: 19 (0.91 seconds after request)\n",
      "Text received: , (0.93 seconds after request)\n",
      "Text received: 20 (0.94 seconds after request)\n",
      "Text received: , (0.97 seconds after request)\n",
      "Text received: 21 (0.98 seconds after request)\n",
      "Text received: , (0.99 seconds after request)\n",
      "Text received: 22 (1.01 seconds after request)\n",
      "Text received: , (1.02 seconds after request)\n",
      "Text received: 23 (1.04 seconds after request)\n",
      "Text received: , (1.05 seconds after request)\n",
      "Text received: 24 (1.07 seconds after request)\n",
      "Text received: , (1.08 seconds after request)\n",
      "Text received: 25 (1.11 seconds after request)\n",
      "Text received: , (1.16 seconds after request)\n",
      "Text received: 26 (1.19 seconds after request)\n",
      "Text received: , (1.22 seconds after request)\n",
      "Text received: 27 (1.24 seconds after request)\n",
      "Text received: , (1.25 seconds after request)\n",
      "Text received: 28 (1.27 seconds after request)\n",
      "Text received: , (1.29 seconds after request)\n",
      "Text received: 29 (1.30 seconds after request)\n",
      "Text received: , (1.32 seconds after request)\n",
      "Text received: 30 (1.33 seconds after request)\n",
      "Text received: , (1.36 seconds after request)\n",
      "Text received: 31 (1.39 seconds after request)\n",
      "Text received: , (1.43 seconds after request)\n",
      "Text received: 32 (1.45 seconds after request)\n",
      "Text received: , (1.48 seconds after request)\n",
      "Text received: 33 (1.49 seconds after request)\n",
      "Text received: , (1.50 seconds after request)\n",
      "Text received: 34 (1.52 seconds after request)\n",
      "Text received: , (1.54 seconds after request)\n",
      "Text received: 35 (1.56 seconds after request)\n",
      "Text received: , (1.57 seconds after request)\n",
      "Text received: 36 (1.58 seconds after request)\n",
      "Text received: , (1.60 seconds after request)\n",
      "Text received: 37 (1.61 seconds after request)\n",
      "Text received: , (1.63 seconds after request)\n",
      "Text received: 38 (1.64 seconds after request)\n",
      "Text received: , (1.65 seconds after request)\n",
      "Text received: 39 (1.66 seconds after request)\n",
      "Text received: , (1.68 seconds after request)\n",
      "Text received: 40 (1.72 seconds after request)\n",
      "Text received: , (1.75 seconds after request)\n",
      "Text received: 41 (1.77 seconds after request)\n",
      "Text received: , (1.78 seconds after request)\n",
      "Text received: 42 (1.81 seconds after request)\n",
      "Text received: , (1.85 seconds after request)\n",
      "Text received: 43 (1.87 seconds after request)\n",
      "Text received: , (1.89 seconds after request)\n",
      "Text received: 44 (1.95 seconds after request)\n",
      "Text received: , (1.97 seconds after request)\n",
      "Text received: 45 (1.99 seconds after request)\n",
      "Text received: , (2.00 seconds after request)\n",
      "Text received: 46 (2.02 seconds after request)\n",
      "Text received: , (2.03 seconds after request)\n",
      "Text received: 47 (2.05 seconds after request)\n",
      "Text received: , (2.07 seconds after request)\n",
      "Text received: 48 (2.08 seconds after request)\n",
      "Text received: , (2.10 seconds after request)\n",
      "Text received: 49 (2.13 seconds after request)\n",
      "Text received: , (2.15 seconds after request)\n",
      "Text received: 50 (2.17 seconds after request)\n",
      "Text received: , (2.20 seconds after request)\n",
      "Text received: 51 (2.22 seconds after request)\n",
      "Text received: , (2.24 seconds after request)\n",
      "Text received: 52 (2.25 seconds after request)\n",
      "Text received: , (2.27 seconds after request)\n",
      "Text received: 53 (2.28 seconds after request)\n",
      "Text received: , (2.31 seconds after request)\n",
      "Text received: 54 (2.32 seconds after request)\n",
      "Text received: , (2.33 seconds after request)\n",
      "Text received: 55 (2.35 seconds after request)\n",
      "Text received: , (2.37 seconds after request)\n",
      "Text received: 56 (2.38 seconds after request)\n",
      "Text received: , (2.40 seconds after request)\n",
      "Text received: 57 (2.42 seconds after request)\n",
      "Text received: , (2.43 seconds after request)\n",
      "Text received: 58 (2.46 seconds after request)\n",
      "Text received: , (2.47 seconds after request)\n",
      "Text received: 59 (2.48 seconds after request)\n",
      "Text received: , (2.50 seconds after request)\n",
      "Text received: 60 (2.51 seconds after request)\n",
      "Text received: , (2.53 seconds after request)\n",
      "Text received: 61 (2.55 seconds after request)\n",
      "Text received: , (2.57 seconds after request)\n",
      "Text received: 62 (2.58 seconds after request)\n",
      "Text received: , (2.61 seconds after request)\n",
      "Text received: 63 (2.62 seconds after request)\n",
      "Text received: , (2.63 seconds after request)\n",
      "Text received: 64 (2.65 seconds after request)\n",
      "Text received: , (2.66 seconds after request)\n",
      "Text received: 65 (2.68 seconds after request)\n",
      "Text received: , (2.69 seconds after request)\n",
      "Text received: 66 (2.71 seconds after request)\n",
      "Text received: , (2.73 seconds after request)\n",
      "Text received: 67 (2.74 seconds after request)\n",
      "Text received: , (2.79 seconds after request)\n",
      "Text received: 68 (2.84 seconds after request)\n",
      "Text received: , (2.87 seconds after request)\n",
      "Text received: 69 (2.91 seconds after request)\n",
      "Text received: , (2.94 seconds after request)\n",
      "Text received: 70 (2.95 seconds after request)\n",
      "Text received: , (2.97 seconds after request)\n",
      "Text received: 71 (2.98 seconds after request)\n",
      "Text received: , (3.00 seconds after request)\n",
      "Text received: 72 (3.02 seconds after request)\n",
      "Text received: , (3.03 seconds after request)\n",
      "Text received: 73 (3.05 seconds after request)\n",
      "Text received: , (3.07 seconds after request)\n",
      "Text received: 74 (3.08 seconds after request)\n",
      "Text received: , (3.11 seconds after request)\n",
      "Text received: 75 (3.14 seconds after request)\n",
      "Text received: , (3.16 seconds after request)\n",
      "Text received: 76 (3.18 seconds after request)\n",
      "Text received: , (3.34 seconds after request)\n",
      "Text received: 77 (3.46 seconds after request)\n",
      "Text received: , (3.55 seconds after request)\n",
      "Text received: 78 (3.60 seconds after request)\n",
      "Text received: , (3.63 seconds after request)\n",
      "Text received: 79 (3.66 seconds after request)\n",
      "Text received: , (3.68 seconds after request)\n",
      "Text received: 80 (3.70 seconds after request)\n",
      "Text received: , (3.73 seconds after request)\n",
      "Text received: 81 (3.77 seconds after request)\n",
      "Text received: , (3.81 seconds after request)\n",
      "Text received: 82 (3.84 seconds after request)\n",
      "Text received: , (3.86 seconds after request)\n",
      "Text received: 83 (3.89 seconds after request)\n",
      "Text received: , (3.92 seconds after request)\n",
      "Text received: 84 (3.95 seconds after request)\n",
      "Text received: , (3.97 seconds after request)\n",
      "Text received: 85 (3.99 seconds after request)\n",
      "Text received: , (4.01 seconds after request)\n",
      "Text received: 86 (4.03 seconds after request)\n",
      "Text received: , (4.06 seconds after request)\n",
      "Text received: 87 (4.07 seconds after request)\n",
      "Text received: , (4.09 seconds after request)\n",
      "Text received: 88 (4.11 seconds after request)\n",
      "Text received: , (4.13 seconds after request)\n",
      "Text received: 89 (4.15 seconds after request)\n",
      "Text received: , (4.16 seconds after request)\n",
      "Text received: 90 (4.19 seconds after request)\n",
      "Text received: , (4.23 seconds after request)\n",
      "Text received: 91 (4.26 seconds after request)\n",
      "Text received: , (4.29 seconds after request)\n",
      "Text received: 92 (4.33 seconds after request)\n",
      "Text received: , (4.36 seconds after request)\n",
      "Text received: 93 (4.38 seconds after request)\n",
      "Text received: , (4.41 seconds after request)\n",
      "Text received: 94 (4.45 seconds after request)\n",
      "Text received: , (4.48 seconds after request)\n",
      "Text received: 95 (4.56 seconds after request)\n",
      "Text received: , (4.56 seconds after request)\n",
      "Text received: 96 (4.56 seconds after request)\n",
      "Text received: , (4.56 seconds after request)\n",
      "Text received: 97 (4.57 seconds after request)\n",
      "Text received: , (4.62 seconds after request)\n",
      "Text received: 98 (4.62 seconds after request)\n",
      "Text received: , (4.63 seconds after request)\n",
      "Text received: 99 (4.65 seconds after request)\n",
      "Text received: , (4.66 seconds after request)\n",
      "Text received: 100 (4.68 seconds after request)\n",
      "Full response received 4.68 seconds after request\n",
      "Full text received: 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100\n"
     ]
    }
   ],
   "source": [
    "# Example of an OpenAI Completion request, using the stream=True option\n",
    "# https://beta.openai.com/docs/api-reference/completions/create\n",
    "\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a Completion request to count to 100\n",
    "response = openai.Completion.create(\n",
    "    model='text-davinci-002',\n",
    "    prompt='1,2,3,',\n",
    "    max_tokens=193,\n",
    "    temperature=0,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "# create variables to collect the stream of events\n",
    "collected_events = []\n",
    "completion_text = ''\n",
    "# iterate through the stream of events\n",
    "for event in response:\n",
    "    event_time = time.time() - start_time  # calculate the time delay of the event\n",
    "    collected_events.append(event)  # save the event response\n",
    "    event_text = event['choices'][0]['text']  # extract the text\n",
    "    completion_text += event_text  # append the text\n",
    "    print(f\"Text received: {event_text} ({event_time:.2f} seconds after request)\")  # print the delay and text\n",
    "\n",
    "# print the time delay and text received\n",
    "print(f\"Full response received {event_time:.2f} seconds after request\")\n",
    "print(f\"Full text received: {completion_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Time comparison\n",
    "\n",
    "In the example above, both requests took about 3 seconds to fully complete. Request times will vary depending on load and other stochastic factors.\n",
    "\n",
    "However, with the streaming request, we received the first token after 0.18 seconds, and subsequent tokens every ~0.01-0.02 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
