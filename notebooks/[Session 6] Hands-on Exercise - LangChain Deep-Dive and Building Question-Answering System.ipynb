{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bb748f",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction to LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af152ea7",
   "metadata": {
    "hidden": true,
    "id": "AWGzucuFfbBn"
   },
   "source": [
    "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
    "\n",
    "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
    "\n",
    "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
    "\n",
    "* **LLMs**: Large language models like GPT-3, BLOOM, etc\n",
    "\n",
    "* **Agents**: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
    "\n",
    "* **Memory**: Short-term memory, long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c49c0b7",
   "metadata": {
    "hidden": true,
    "id": "r-ryCeG_f_GC"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee902102",
   "metadata": {
    "hidden": true,
    "id": "mNaXrEPOhbuL"
   },
   "source": [
    "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
    "\n",
    "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291d6b3c-54b0-439b-8a78-905fcc064f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc7a01",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "YpdXG9YtzrLJ"
   },
   "source": [
    "### Using OpenAI LLMs in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eff9f4",
   "metadata": {
    "hidden": true,
    "id": "0fOo9qQvDgkz"
   },
   "source": [
    "We can also use OpenAI's generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d01dc8",
   "metadata": {
    "hidden": true,
    "id": "2AWnaTCP0Ryg"
   },
   "source": [
    "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61bd5d2",
   "metadata": {
    "hidden": true,
    "id": "ZhQSDoYe0ly4"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb8024",
   "metadata": {
    "hidden": true,
    "id": "_NvK4o6SDrs0"
   },
   "source": [
    "Alternatively if using Azure OpenAI we do:\n",
    "\n",
    "```python\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"your-azure-deployment\", \n",
    "    model_name=\"text-davinci-003\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31498ae3-b898-40dc-86d0-86f882fa0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build prompt template for simple question-answering\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f27776",
   "metadata": {
    "hidden": true,
    "id": "SGL2zs3uEVj6"
   },
   "source": [
    "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98de731d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "gVSsC3iGEPAp",
    "outputId": "1d562f8d-2fbf-4cc4-84cd-cce998720eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d35d74",
   "metadata": {
    "hidden": true,
    "id": "DL-buasOKpKs"
   },
   "source": [
    "The same works again for multiple questions using `generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1d31f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "vMua1MWcKtSx",
    "outputId": "55efeae1-8c30-4069-a2a8-fb78212f8523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 6 ft 4 inches is approximately 193.04 centimeters.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' The 12th person on the moon was Charles \"Pete\" Conrad, Jr. He was part of the Apollo 12 mission in 1969.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 62, 'total_tokens': 137, 'prompt_tokens': 75}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('8b49924e-daf7-4742-9667-1242b50f451a')), RunInfo(run_id=UUID('d8064431-1f56-4fec-ad50-f65d81634670')), RunInfo(run_id=UUID('42c1ecd1-4bbf-4f40-bd8c-15757d0c01fb')), RunInfo(run_id=UUID('4a270645-7623-4c6d-9587-5a8abcb7c7cf'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4d8dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713cf24a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "w2-es7SgFddS",
    "outputId": "d16b7afc-f0d1-4f6a-9d89-f6811839bb02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. The Green Bay Packers \n",
      "2. 193 centimeters \n",
      "3. Harrison Schmitt \n",
      "4. None\n"
     ]
    }
   ],
   "source": [
    "qs = [\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
    "    \"Who was the 12th person on the moon?\",\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    "]\n",
    "print(llm_chain.run(qs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c2dfe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45322786",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "xbjxnnVzA47s",
    "outputId": "cf5397ca-9e06-4221-eb45-17b1346f6f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
      "If you are 6 ft 4 inches, you are 193.04 centimeters tall.\n",
      "The 12th person on the moon was Harrison Schmitt.\n",
      "A blade of grass does not have eyes.\n"
     ]
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(\n",
    "    template=multi_template,\n",
    "    input_variables=[\"questions\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=davinci\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
    "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
    "    \"Who was the 12th person on the moon?\" +\n",
    "    \"How many eyes does a blade of grass have?\"\n",
    ")\n",
    "\n",
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b49af",
   "metadata": {
    "hidden": true,
    "id": "ybMkI18xfbBr"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f2cd2b",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Fundamentals of Prompt Engineering with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d79c32",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !pip install langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935b49d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Structure of a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514151b1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A prompt can consist of multiple components:\n",
    "\n",
    "* Instructions\n",
    "* External information or context\n",
    "* User input or query\n",
    "* Output indicator\n",
    "\n",
    "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
    "\n",
    "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
    "\n",
    "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
    "\n",
    "**User input or query** is typically a query directly input by the user of the system.\n",
    "\n",
    "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
    "\n",
    "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d642bfb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121f23e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we have:\n",
    "\n",
    "```\n",
    "Instructions\n",
    "\n",
    "Context\n",
    "\n",
    "Question (user input)\n",
    "\n",
    "Output indicator (\"Answer: \")\n",
    "```\n",
    "\n",
    "Let's try sending this to a GPT-3 model. We will use the LangChain library but you can also use the `openai` library directly. In both cases, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
    "\n",
    "We initialize a `text-davinci-003` model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55f42fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b66a3d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And make a generation from our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f89ef4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "print(openai(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341aa634",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a `PromptTemplate` with a single input variable `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b366d6b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bccd51",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can insert the user's `query` to the prompt template via the `query` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bddb6d96",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4501a2a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699820ed",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is just a simple implementation, that we can easily replace with f-strings (like `f\"insert some custom text '{custom_text}' etc\"`). But using LangChain's `PromptTemplate` object we're able to formalize the process, add multiple parameters, and build the prompts in an object-oriented way.\n",
    "\n",
    "Yet, these are not the only benefits of using LangChains prompt tooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc037ad5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Few Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b291b3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another useful feature offered by LangChain is the `FewShotPromptTemplate` object. This is ideal for what we'd call *few-shot learning* using our prompts.\n",
    "\n",
    "To give some context, the primary sources of \"knowledge\" for LLMs are:\n",
    "\n",
    "* **Parametric knowledge** — the knowledge has been learned during model training and is stored within the model weights.\n",
    "\n",
    "* **Source knowledge** — the knowledge is provided within model input at inference time, i.e. via the prompt.\n",
    "\n",
    "The idea behind `FewShotPromptTemplate` is to provide few-shot training as **source knowledge**. To do this we add a few examples to our prompts that the model can read and then apply to our user's input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c48c1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Few-shot Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e923a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe7505e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The meaning of life is to find your own meaning. Live life to the fullest!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "openai.temperature = 1.0  # increase creativity/randomness of output\n",
    "\n",
    "print(openai(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d130e7e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cc33c2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find something worth living for.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "print(openai(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00bae5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now get a much better response and we did this via *few-shot learning* by adding a few examples via our source knowledge.\n",
    "\n",
    "Now, to implement this with LangChain's `FewShotPromptTemplate` we need to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50211f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f440177",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's see what this creates when we feed in a user query..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ede717",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b2952",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And to generate with this we just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ade46775",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The meaning of life is up for debate, but having a good time is a safe bet.\n"
     ]
    }
   ],
   "source": [
    "print(openai(\n",
    "    few_shot_prompt_template.format(query=query)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda53e47",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, another good response.\n",
    "\n",
    "However, this does some somewhat convoluted. Why go through all of the above with `FewShotPromptTemplate`, the `examples` dictionary, etc — when we can do the same with a single f-string.\n",
    "\n",
    "Well this approach is more robust and contains some nice features. One of those is the ability to include or exclude examples based on the length of our query.\n",
    "\n",
    "This is actually very important because the max length of our prompt and generation output is limited. This limitation is the *max context window*, and is simply the length of our prompt + length of our generation (which we define via `max_tokens`).\n",
    "\n",
    "So we must try to maximize the number of examples we give to the model as few-shot learning examples, while ensuring we don't exceed the maximum context window or increase processing times excessively.\n",
    "\n",
    "Let's see how the dynamic inclusion/exclusion of examples works. First we need more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c5cdb7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
    "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite color?\",\n",
    "        \"answer\": \"79\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite food?\",\n",
    "        \"answer\": \"Carbon based lifeforms\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"What is the best thing in the world?\",\n",
    "        \"answer\": \"The perfect pizza.\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"If you could do anything in the world what would you do?\",\n",
    "        \"answer\": \"Take over the world, of course!\"\n",
    "    }, {\n",
    "        \"query\": \"Where should I travel?\",\n",
    "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d484e41",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then rather than using the `examples` list of dictionaries directly we use a `LengthBasedExampleSelector` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17f2dbec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # this sets the max length that examples should be\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb068ac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that the `max_length` is measured as a split of words between newlines and spaces, determined by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e52ee155",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
    "\n",
    "words = re.split('[\\n ]', some_text)\n",
    "print(words, len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d0ef8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we use the selector to initialize a `dynamic_prompt_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb56016b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196e4e9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see that the number of included prompts will vary based on the length of our query..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8240fac6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9f0ad1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Magic, duh!\n"
     ]
    }
   ],
   "source": [
    "query = \"How do birds fly?\"\n",
    "\n",
    "print(openai(\n",
    "    dynamic_prompt_template.format(query=query)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32af070",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Or if we ask a longer question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38d744e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7ed01",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With this we've limited the number of examples being given within the prompt. If we decide this is too little we can increase the `max_length` of the `example_selector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74af348c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: What is the weather like today?\n",
      "AI: Cloudy with a chance of memes.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=100  # increased max length\n",
    ")\n",
    "\n",
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f90a8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These are just a few of the prompt tooling available in LangChain. For example, there is actually an entire other set of example selectors beyond the `LengthBasedExampleSelector`. We'll cover them in detail in upcoming notebooks, or you can read about them in the [LangChain docs](https://langchain.readthedocs.io/en/latest/modules/prompts/examples/example_selectors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba785a3f",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LangChain Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ba72d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n",
    "\n",
    "The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain — a `PromptTemplate` — to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain — a LLM.\n",
    "\n",
    "We'll start by importing all the libraries that we'll be using in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66fb9c2a",
   "metadata": {
    "hidden": true,
    "id": "66fb9c2a"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wPdWz1IdxyBR",
   "metadata": {
    "hidden": true,
    "id": "wPdWz1IdxyBR"
   },
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baaa74b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309g_2pqxzzB",
   "metadata": {
    "hidden": true,
    "id": "309g_2pqxzzB"
   },
   "source": [
    "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "DsC3szr6yP3L",
   "metadata": {
    "hidden": true,
    "id": "DsC3szr6yP3L"
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f31b4",
   "metadata": {
    "hidden": true,
    "id": "6e1f31b4"
   },
   "source": [
    "### What are chains anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b919c3a",
   "metadata": {
    "hidden": true,
    "id": "5b919c3a"
   },
   "source": [
    "**Definition**: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n",
    "\n",
    "The official definition of chains is the following:\n",
    "\n",
    "\n",
    "> A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains.\n",
    "\n",
    "\n",
    "So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4644b2f",
   "metadata": {
    "hidden": true,
    "id": "c4644b2f"
   },
   "source": [
    "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
    "\n",
    "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
    "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d283b6",
   "metadata": {
    "hidden": true,
    "id": "e4d283b6"
   },
   "source": [
    "Let's take a peek into what these chains have to offer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831827b7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "831827b7"
   },
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66e4b4",
   "metadata": {
    "hidden": true,
    "id": "6c66e4b4"
   },
   "source": [
    "Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HF3XCWD2sVi0",
   "metadata": {
    "hidden": true,
    "id": "HF3XCWD2sVi0"
   },
   "source": [
    "##### Pro-tip: use `verbose=True` to see what the different steps in the chain are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4161561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "b4161561",
    "outputId": "16486831-80a5-41df-906e-376575b7f514"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "13**.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 264 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198eebb2",
   "metadata": {
    "hidden": true,
    "id": "198eebb2"
   },
   "source": [
    "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0821a",
   "metadata": {
    "hidden": true,
    "id": "a7a0821a"
   },
   "source": [
    "**Enter prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c5798",
   "metadata": {
    "hidden": true,
    "id": "c86c5798"
   },
   "source": [
    "The question we send as input to the chain is not the only input that the llm recieves 😉. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62778ef4",
   "metadata": {
    "hidden": true,
    "id": "62778ef4",
    "outputId": "211670a8-db56-4f68-d873-97d279870890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708031d8",
   "metadata": {
    "hidden": true,
    "id": "708031d8"
   },
   "source": [
    "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! 🧐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66b92768",
   "metadata": {
    "hidden": true,
    "id": "66b92768",
    "outputId": "6c9b7f59-529d-409e-8562-5a622f326473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 17 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n2.907'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147e7bf",
   "metadata": {
    "hidden": true,
    "id": "d147e7bf"
   },
   "source": [
    "Wrong answer! Herein lies the power of prompting and one of our most important insights so far: \n",
    "\n",
    "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2a31f",
   "metadata": {
    "hidden": true,
    "id": "1cd2a31f"
   },
   "source": [
    "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3488c5b6",
   "metadata": {
    "hidden": true,
    "id": "3488c5b6",
    "outputId": "8b32a998-8e11-48bf-f21c-f5d086186508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, str],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
      "        _run_manager.on_text(inputs[self.input_key])\n",
      "        llm_output = self.llm_chain.predict(\n",
      "            question=inputs[self.input_key],\n",
      "            stop=[\"```output\"],\n",
      "            callbacks=_run_manager.get_child(),\n",
      "        )\n",
      "        return self._process_llm_result(llm_output, _run_manager)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b6c2e",
   "metadata": {
    "hidden": true,
    "id": "fa6b6c2e"
   },
   "source": [
    "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f96bd3",
   "metadata": {
    "hidden": true,
    "id": "67f96bd3"
   },
   "source": [
    "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109619a",
   "metadata": {
    "hidden": true,
    "id": "b109619a"
   },
   "source": [
    "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly 🔥\n",
    "\n",
    "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html) and tinker with the example notebooks that you might find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e329c",
   "metadata": {
    "hidden": true,
    "id": "381e329c"
   },
   "source": [
    "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a25a2",
   "metadata": {
    "hidden": true,
    "id": "f66a25a2"
   },
   "source": [
    "### Generic chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b32a84",
   "metadata": {
    "hidden": true,
    "id": "70b32a84"
   },
   "source": [
    "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e2048",
   "metadata": {
    "hidden": true,
    "id": "4b8e2048"
   },
   "source": [
    "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat 😉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e778d2",
   "metadata": {
    "hidden": true,
    "id": "a6e778d2"
   },
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c794e00a",
   "metadata": {
    "hidden": true,
    "id": "c794e00a"
   },
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc1ac6",
   "metadata": {
    "hidden": true,
    "id": "42dc1ac6"
   },
   "source": [
    "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "286f7295",
   "metadata": {
    "hidden": true,
    "id": "286f7295"
   },
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "977bf11a",
   "metadata": {
    "hidden": true,
    "id": "977bf11a",
    "outputId": "8d6eaa5e-b417-4c17-a345-7b7e32071430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f84cd0",
   "metadata": {
    "hidden": true,
    "id": "b3f84cd0"
   },
   "source": [
    "Great! Now things will get interesting.\n",
    "\n",
    "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77042a",
   "metadata": {
    "hidden": true,
    "id": "5b77042a"
   },
   "source": [
    "First we will build the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73719a5d",
   "metadata": {
    "hidden": true,
    "id": "73719a5d"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2ec83",
   "metadata": {
    "hidden": true,
    "id": "83b2ec83"
   },
   "source": [
    "And next, initialize our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48a067ab",
   "metadata": {
    "hidden": true,
    "id": "48a067ab"
   },
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324005d",
   "metadata": {
    "hidden": true,
    "id": "2324005d"
   },
   "source": [
    "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
    "\n",
    "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da4925",
   "metadata": {
    "hidden": true,
    "id": "c5da4925"
   },
   "source": [
    "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06f51f17",
   "metadata": {
    "hidden": true,
    "id": "06f51f17"
   },
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f51d8",
   "metadata": {
    "hidden": true,
    "id": "7f0f51d8"
   },
   "source": [
    "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8032489",
   "metadata": {
    "hidden": true,
    "id": "a8032489"
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f55d21",
   "metadata": {
    "hidden": true,
    "id": "b2f55d21"
   },
   "source": [
    "We are all set. Time to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d507aa5c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 163 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nChains let us link up multiple pieces to make one dope app. Like, we can take user input, style it up with a PromptTemplate, then pass it to an LLM. We can get even more creative by combining multiple chains or mixin' chains with other components.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b52e19",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "60b52e19"
   },
   "source": [
    "### A note on langchain-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f649da",
   "metadata": {
    "hidden": true,
    "id": "02f649da"
   },
   "source": [
    "`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "411500c2",
   "metadata": {
    "hidden": true,
    "id": "411500c2"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b375e5b7",
   "metadata": {
    "hidden": true,
    "id": "b375e5b7"
   },
   "source": [
    "Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbe8748d",
   "metadata": {
    "hidden": true,
    "id": "fbe8748d"
   },
   "outputs": [],
   "source": [
    "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfe67c",
   "metadata": {
    "hidden": true,
    "id": "ebcfe67c"
   },
   "source": [
    "What if we want to change some of the configuration parameters? We can simply override it after loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0d54233",
   "metadata": {
    "hidden": true,
    "id": "d0d54233",
    "outputId": "92eba1cf-e47b-4df1-cc51-caee5a6a720b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "074f8806",
   "metadata": {
    "hidden": true,
    "id": "074f8806"
   },
   "outputs": [],
   "source": [
    "llm_math_chain.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "465a6cbf",
   "metadata": {
    "hidden": true,
    "id": "465a6cbf",
    "outputId": "0207bf08-0db0-4d85-e3b9-ac7922f344c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc688ca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's it for this example on chains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ddfba",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Conversational Memory with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcqKO0aI6_PI",
   "metadata": {
    "hidden": true,
    "id": "hcqKO0aI6_PI"
   },
   "source": [
    "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
    "\n",
    "The memory allows a _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
    "\n",
    "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
    "\n",
    "In this notebook we'll explore this form of memory in the context of the LangChain library.\n",
    "\n",
    "We'll start by importing all of the libraries that we'll be using in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "07c3fc35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "uZR3iGJJtdDE",
    "outputId": "98873b1a-5688-4f64-c400-e17be707c56b"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e99e024",
   "metadata": {
    "hidden": true,
    "id": "66fb9c2a"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "                                                  ConversationSummaryMemory, \n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f87cb",
   "metadata": {
    "hidden": true,
    "id": "wPdWz1IdxyBR"
   },
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4498596",
   "metadata": {
    "hidden": true,
    "id": "baaa74b8"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    model_name='text-davinci-003'  # can be used with llms like 'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc08e6",
   "metadata": {
    "hidden": true,
    "id": "309g_2pqxzzB"
   },
   "source": [
    "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4568c2ad",
   "metadata": {
    "hidden": true,
    "id": "DsC3szr6yP3L"
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CnNF6i9r8RY_",
   "metadata": {
    "hidden": true,
    "id": "CnNF6i9r8RY_"
   },
   "source": [
    "Now let's dive into **Conversational Memory**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f915b2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "6e1f31b4"
   },
   "source": [
    "## What is memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5492f8",
   "metadata": {
    "hidden": true,
    "id": "5b919c3a"
   },
   "source": [
    "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
    "\n",
    "The official definition of memory is the following:\n",
    "\n",
    "\n",
    "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of “Memory” exists to do exactly that.\n",
    "\n",
    "\n",
    "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343a0e2",
   "metadata": {
    "hidden": true,
    "id": "3343a0e2"
   },
   "source": [
    "Before we delve into the different memory modules that the library offers, we will introduce the chain we will be using for these examples: the `ConversationChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c13e9",
   "metadata": {
    "hidden": true,
    "id": "6c9c13e9"
   },
   "source": [
    "As always, when understanding a chain it is interesting to peek into its prompt first and then take a look at its `._call` method. As we saw in the chapter on chains, we can check out the prompt by accessing the `template` within the `prompt` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96ff1ce3",
   "metadata": {
    "hidden": true,
    "id": "96ff1ce3"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90ad394d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "90ad394d",
    "outputId": "1c641d37-b3e7-40d5-815b-936fcd2d9a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b1e0c",
   "metadata": {
    "hidden": true,
    "id": "9f8b1e0c"
   },
   "source": [
    "Interesting! So this chain's prompt is telling it to chat with the user and try to give truthful answers. If we look closely, there is a new component in the prompt that we didn't see when we were tinkering with the `LLMMathChain`: _history_. This is where our memory will come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e7770",
   "metadata": {
    "hidden": true,
    "id": "4a7e7770"
   },
   "source": [
    "What is this chain doing with this prompt? Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43bfd2da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "43bfd2da",
    "outputId": "489437a5-0f0b-412a-f817-f0df817211c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, Any],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        response = self.generate([inputs], run_manager=run_manager)\n",
      "        return self.create_outputs(response)[0]\n",
      "     def apply(\n",
      "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
      "    ) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        callback_manager = CallbackManager.configure(\n",
      "            callbacks, self.callbacks, self.verbose\n",
      "        )\n",
      "        run_manager = callback_manager.on_chain_start(\n",
      "            dumpd(self),\n",
      "            {\"input_list\": input_list},\n",
      "        )\n",
      "        try:\n",
      "            response = self.generate(input_list, run_manager=run_manager)\n",
      "        except (KeyboardInterrupt, Exception) as e:\n",
      "            run_manager.on_chain_error(e)\n",
      "            raise e\n",
      "        outputs = self.create_outputs(response)\n",
      "        run_manager.on_chain_end({\"outputs\": outputs})\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e664af",
   "metadata": {
    "hidden": true,
    "id": "84e664af"
   },
   "source": [
    "Nothing really magical going on here, just a straightforward pass through an LLM. In fact, this chain inherits these methods directly from the `LLMChain` without any modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8f4aa79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "d8f4aa79",
    "outputId": "ca3413ec-1ceb-4160-f6e9-2031350780a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, Any],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        response = self.generate([inputs], run_manager=run_manager)\n",
      "        return self.create_outputs(response)[0]\n",
      "     def apply(\n",
      "        self, input_list: List[Dict[str, Any]], callbacks: Callbacks = None\n",
      "    ) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        callback_manager = CallbackManager.configure(\n",
      "            callbacks, self.callbacks, self.verbose\n",
      "        )\n",
      "        run_manager = callback_manager.on_chain_start(\n",
      "            dumpd(self),\n",
      "            {\"input_list\": input_list},\n",
      "        )\n",
      "        try:\n",
      "            response = self.generate(input_list, run_manager=run_manager)\n",
      "        except (KeyboardInterrupt, Exception) as e:\n",
      "            run_manager.on_chain_error(e)\n",
      "            raise e\n",
      "        outputs = self.create_outputs(response)\n",
      "        run_manager.on_chain_end({\"outputs\": outputs})\n",
      "        return outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa70bf",
   "metadata": {
    "hidden": true,
    "id": "6aaa70bf"
   },
   "source": [
    "So basically this chain combines an input from the user with the conversation history to generate a meaningful (and hopefully truthful) response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5172f",
   "metadata": {
    "hidden": true,
    "id": "19f5172f"
   },
   "source": [
    "Now that we've understood the basics of the chain we'll be using, we can get into memory. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a33f6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "0f1a33f6"
   },
   "source": [
    "## Memory types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d732b7a",
   "metadata": {
    "hidden": true,
    "id": "4d732b7a"
   },
   "source": [
    "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d70642",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "04d70642"
   },
   "source": [
    "### Memory type #1: ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3cb2b",
   "metadata": {
    "hidden": true,
    "id": "53d3cb2b"
   },
   "source": [
    "The `ConversationBufferMemory` does just what its name suggests: it keeps a buffer of the previous conversation excerpts as part of the context in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a974a",
   "metadata": {
    "hidden": true,
    "id": "d80a974a"
   },
   "source": [
    "**Key feature:** _the conversation buffer memory keeps the previous pieces of conversation completely unmodified, in their raw form._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2267f1f0",
   "metadata": {
    "hidden": true,
    "id": "2267f1f0"
   },
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lseziAMcAyvX",
   "metadata": {
    "hidden": true,
    "id": "lseziAMcAyvX"
   },
   "source": [
    "We pass a user prompt the the `ConversationBufferMemory` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "M0cwooC5A5Id",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "M0cwooC5A5Id",
    "outputId": "8a8178eb-b9ac-45cf-baed-255b413b0630"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today, isn't it? How can I help you?\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlKINTFYA9eo",
   "metadata": {
    "hidden": true,
    "id": "xlKINTFYA9eo"
   },
   "source": [
    "This one call used a total of `85` tokens, but we can't see that from the above. If we'd like to count the number of tokens being used we just pass our conversation chain object and the message we'd like to input via the `count_tokens` function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d1bd5a88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "hidden": true,
    "id": "d1bd5a88",
    "outputId": "cb593afd-7efd-4c0e-cf04-82dc1a324aff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "146170ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "hidden": true,
    "id": "146170ca",
    "outputId": "dbb6f78c-b169-463e-c1c8-a35151894f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 264 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e15411a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "hidden": true,
    "id": "3e15411a",
    "outputId": "f6857844-ee6f-49ef-df50-54335f248bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 344 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Data sources that could be used to give context to the model include text corpora, images, audio, and video. Text corpora can provide additional context to the model by providing additional training data. Images, audio, and video can provide additional context by providing additional visual or auditory information.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3352cc48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "hidden": true,
    "id": "3352cc48",
    "outputId": "62294954-cc7e-4ef3-e5fc-19a5c4ffc4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 372 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your aim is to explore the potential of integrating Large Language Models with external knowledge.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b74ff",
   "metadata": {
    "hidden": true,
    "id": "431b74ff"
   },
   "source": [
    "Our LLM with `ConversationBufferMemory` can clearly remember earlier interactions in the conversation. Let's take a closer look to how the LLM is saving our previous conversation. We can do this by accessing the `.buffer` attribute for the `.memory` in our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "984afd09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "984afd09",
    "outputId": "4233d17f-1001-48e5-d256-0595e00dbf40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?\n",
      "Human: I just want to analyze the different possibilities. What can you think of?\n",
      "AI:  There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.\n",
      "Human: Which data source types could be used to give context to the model?\n",
      "AI:   Data sources that could be used to give context to the model include text corpora, images, audio, and video. Text corpora can provide additional context to the model by providing additional training data. Images, audio, and video can provide additional context by providing additional visual or auditory information.\n",
      "Human: What is my aim again?\n",
      "AI:  Your aim is to explore the potential of integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570267d",
   "metadata": {
    "hidden": true,
    "id": "4570267d"
   },
   "source": [
    "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1a90b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "acf1a90b"
   },
   "source": [
    "### Memory type #2: ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f61fe9",
   "metadata": {
    "hidden": true,
    "id": "01f61fe9"
   },
   "source": [
    "The problem with the `ConversationBufferMemory` is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516c7d4",
   "metadata": {
    "hidden": true,
    "id": "0516c7d4"
   },
   "source": [
    "Enter `ConversationSummaryMemory`.\n",
    "\n",
    "Again, we can infer from the name what is going on.. we will keep a summary of our previous conversation snippets as our history. How will we summarize these? LLM to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0a905",
   "metadata": {
    "hidden": true,
    "id": "86b0a905"
   },
   "source": [
    "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized form, where the summarization is performed by an LLM._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6050c",
   "metadata": {
    "hidden": true,
    "id": "0ea6050c"
   },
   "source": [
    "In this case we need to send the llm to our memory constructor to power its summarization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f33a16a7",
   "metadata": {
    "hidden": true,
    "id": "f33a16a7"
   },
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c4896",
   "metadata": {
    "hidden": true,
    "id": "b64c4896"
   },
   "source": [
    "When we have an llm, we always have a prompt ;) Let's see what's going on inside our conversation summary memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c476824d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "c476824d",
    "outputId": "282be20e-9048-4f37-fc89-8a7eb8dfe1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90cdf3",
   "metadata": {
    "hidden": true,
    "id": "df90cdf3"
   },
   "source": [
    "Cool! So each new interaction is summarized and appended to a running summary as the memory of our chain. Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34343665",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "hidden": true,
    "id": "34343665",
    "outputId": "ac04f6bc-9dcb-446c-d4b9-8fd2311d605e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 290 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b757bba3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "hidden": true,
    "id": "b757bba3",
    "outputId": "9de1823a-0dfe-45ff-fadc-26eff6fdce99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 440 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That sounds like an interesting project! I'm familiar with Large Language Models, but I'm not sure how they could be integrated with external knowledge. Could you tell me more about what you have in mind?\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d0a373e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "hidden": true,
    "id": "d0a373e2",
    "outputId": "d4f561d7-d1c7-45e5-99ba-266130ee67ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 664 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I can think of a few possibilities. One option is to use a large language model to generate a set of candidate answers to a given query, and then use external knowledge to filter out the most relevant answers. Another option is to use the large language model to generate a set of candidate answers, and then use external knowledge to score and rank the answers. Finally, you could use the large language model to generate a set of candidate answers, and then use external knowledge to refine the answers.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e286f0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "hidden": true,
    "id": "2e286f0d",
    "outputId": "9558ef92-5f9c-4818-be8b-1e7e6ec19864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 799 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There are many different types of data sources that could be used to give context to the model. These could include structured data sources such as databases, unstructured data sources such as text documents, or even external APIs that provide access to external knowledge. Additionally, the model could be trained on a combination of these data sources to provide a more comprehensive understanding of the context.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "891180f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "hidden": true,
    "id": "891180f2",
    "outputId": "8035333e-d7c0-4a46-d8b8-acb3501d27e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 853 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your aim is to explore the potential of integrating Large Language Models with external knowledge.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d768e44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "2d768e44",
    "outputId": "3bd42ac9-d56b-45f4-99ac-45cd5a656b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI with a good morning, to which the AI responded with a good morning and asked how it could help. The human expressed interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responded positively and asked for more information. The human asked the AI to think of different possibilities, and the AI suggested three options: using the large language model to generate a set of candidate answers and then using external knowledge to filter out the most relevant answers, score and rank the answers, or refine the answers. The human then asked which data source types could be used to give context to the model, to which the AI responded that there are many different types of data sources that could be used, such as structured data sources, unstructured data sources, or external APIs. Additionally, the model could be trained on a combination of these data sources to provide a more comprehensive understanding of the context. The human then asked what their aim was again, to which the AI responded that their aim was to explore the potential of integrating Large Language Models with external knowledge.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd35c8c",
   "metadata": {
    "hidden": true,
    "id": "0dd35c8c"
   },
   "source": [
    "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
    "\n",
    "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "nzijj4RZFX3I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "nzijj4RZFX3I",
    "outputId": "dc272cbb-acfd-4b4a-f854-8fa63f9732d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 318\n",
      "Summary memory conversation length: 219\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('text-davinci-003')\n",
    "\n",
    "# show number of tokens for the memory used by each memory type\n",
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab0c09",
   "metadata": {
    "hidden": true,
    "id": "2bab0c09"
   },
   "source": [
    "_Practical Note: the `text-davinci-003` and `gpt-3.5-turbo` models [have](https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens) a large max tokens count of 4096 tokens between prompt and answer._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494830ea",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "494830ea"
   },
   "source": [
    "### Memory type #3: ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00762844",
   "metadata": {
    "hidden": true,
    "id": "00762844"
   },
   "source": [
    "Another great option for these cases is the `ConversationBufferWindowMemory` where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably. We will control this window with the `k` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a5915",
   "metadata": {
    "hidden": true,
    "id": "206a5915"
   },
   "source": [
    "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45be373a",
   "metadata": {
    "hidden": true,
    "id": "45be373a"
   },
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc4dd8a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "hidden": true,
    "id": "fc4dd8a0",
    "outputId": "c4ec1cc8-f218-4f7b-e27e-f5fb73e59228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 85 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9992e8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "hidden": true,
    "id": "b9992e8d",
    "outputId": "ac7ae1af-2329-4766-ac5e-8fce24a1d272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 178 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3f2e98d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "hidden": true,
    "id": "3f2e98d9",
    "outputId": "dc60726a-4be2-480f-892b-443da9b2859e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 233 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a2a8d062",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "hidden": true,
    "id": "a2a8d062",
    "outputId": "dbb27cf0-2e87-41d0-a733-68921d250481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 245 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Data sources that could be used to give context to the model include text corpora, structured databases, and ontologies. Text corpora provide a large amount of text data that can be used to train the model and provide additional context. Structured databases provide structured data that can be used to provide additional context to the model. Ontologies provide a structured representation of knowledge that can be used to provide additional context to the model.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Which data source types could be used to give context to the model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff199a3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "hidden": true,
    "id": "ff199a3f",
    "outputId": "81573cf0-7f39-4a8c-8ccd-e79cd80f2523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 186 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your aim is to use data sources to give context to the model.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f59f77",
   "metadata": {
    "hidden": true,
    "id": "f5f59f77"
   },
   "source": [
    "As we can see, it effectively 'fogot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be `1`, we would expect it remembers only the last interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b354c8d",
   "metadata": {
    "hidden": true,
    "id": "6b354c8d"
   },
   "source": [
    "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85266406",
   "metadata": {
    "hidden": true,
    "id": "85266406"
   },
   "outputs": [],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5904ae2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "5904ae2a",
    "outputId": "bd0aa797-7a43-4af5-a531-209aa6272dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my aim again?\n",
      "AI:  Your aim is to use data sources to give context to the model.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b937d",
   "metadata": {
    "hidden": true,
    "id": "ae8b937d"
   },
   "source": [
    "Makes sense. \n",
    "\n",
    "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9fbb50fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9fbb50fe",
    "outputId": "c35dca36-a7c7-4d61-da19-c28173fa8319"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 318\n",
      "Summary memory conversation length: 219\n",
      "Buffer window memory conversation length: 26\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
    "    f'Buffer window memory conversation length: {len(tokenizer.encode(bufw_history))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69842cc1",
   "metadata": {
    "hidden": true,
    "id": "69842cc1"
   },
   "source": [
    "_Practical Note: We are using `k=2` here for illustrative purposes, in most real world applications you would need a higher value for k._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea5fc8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "2aea5fc8"
   },
   "source": [
    "### More memory types!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb5162",
   "metadata": {
    "hidden": true,
    "id": "daeb5162"
   },
   "source": [
    "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0365333",
   "metadata": {
    "hidden": true,
    "id": "f0365333"
   },
   "source": [
    "#### ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f298e",
   "metadata": {
    "hidden": true,
    "id": "317f298e"
   },
   "source": [
    "**Key feature:** _the conversation summary memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef5c8b",
   "metadata": {
    "hidden": true,
    "id": "57ef5c8b"
   },
   "source": [
    "#### ConversationKnowledgeGraphMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40248f03",
   "metadata": {
    "hidden": true,
    "id": "40248f03"
   },
   "source": [
    "This is a super cool memory type that was introduced just [recently](https://twitter.com/LangChainAI/status/1625158388824043522). It is based on the concept of a _knowledge graph_ which recognizes different entities and connects them in pairs with a predicate resulting in (subject, predicate, object) triplets. This enables us to compress a lot of information into highly significant snippets that can be fed into the model as context. If you want to understand this memory type in more depth you can check out [this](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph) blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91952cd1",
   "metadata": {
    "hidden": true,
    "id": "91952cd1"
   },
   "source": [
    "**Key feature:** _the conversation knowledge graph memory keeps a knowledge graph of all the entities that have been mentioned in the interactions together with their semantic relationships._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02241bc3",
   "metadata": {
    "hidden": true,
    "id": "02241bc3"
   },
   "outputs": [],
   "source": [
    "# you may need to install this library\n",
    "# !pip install -qU networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5f10a89",
   "metadata": {
    "hidden": true,
    "id": "c5f10a89"
   },
   "outputs": [],
   "source": [
    "conversation_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "65957fe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "hidden": true,
    "id": "65957fe2",
    "outputId": "c9561a4a-412a-4d92-865d-9e81a09bb101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1168 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Human! My name is AI. It's nice to meet you. I like mangoes too! Did you know that mangoes are a great source of vitamins A and C?\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_kg, \n",
    "    \"My name is human and I like mangoes!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74054534",
   "metadata": {
    "hidden": true,
    "id": "74054534"
   },
   "source": [
    "The memory keeps a knowledge graph of everything it learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a8c54fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "5a8c54fb",
    "outputId": "adf96679-087b-4b77-c00d-9bf9e98f9278"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Human', 'human', 'name is'), ('Human', 'mangoes', 'likes')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_kg.memory.kg.get_triples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1ca15",
   "metadata": {
    "hidden": true,
    "id": "e1a1ca15"
   },
   "source": [
    "#### ConversationEntityMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9aeaf",
   "metadata": {
    "hidden": true,
    "id": "41e9aeaf"
   },
   "source": [
    "**Key feature:** _the conversation entity memory keeps a recollection of the main entities that have been mentioned, together with their specific attributes._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900a385",
   "metadata": {
    "hidden": true,
    "id": "2900a385"
   },
   "source": [
    "The way this works is quite similar to the `ConversationKnowledgeGraphMemory`, you can refer to the [docs](https://langchain.readthedocs.io/en/latest/modules/memory/examples/entity_summary_memory.html) if you want to see it in action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45112bd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "d45112bd"
   },
   "source": [
    "## What else can we do with memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78296bff",
   "metadata": {
    "hidden": true,
    "id": "78296bff"
   },
   "source": [
    "There are several cool things we can do with memory in langchain. We can:\n",
    "* implement our own custom memory module\n",
    "* use multiple memory modules in the same chain\n",
    "* combine agents with memory and other tools\n",
    "\n",
    "If this piques your interest, we suggest you to go take a look at the memory [how-to](https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html) section in the docs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe2121",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "ytU8AV9uMty3"
   },
   "source": [
    "## Extra Material: Token Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6403e8fb",
   "metadata": {
    "hidden": true,
    "id": "ytU8AV9uMty3"
   },
   "source": [
    "This is an additional piece of material alongside the [LangChain Handbook notebook on Conversational Memory](https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/03-langchain-conversational-memory.ipynb).\n",
    "\n",
    "In this notebook we will count the number of tokens used in a conversation for different conversational memory types.\n",
    "\n",
    "We begin by installing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1d50a3cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9iK0npZ4Mgae",
    "outputId": "f79e4b83-b1f2-4610-b032-6fbb042c3872"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU langchain openai transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ad81e",
   "metadata": {
    "hidden": true,
    "id": "g6-xDqLGRYsh"
   },
   "source": [
    "Import required libraries and objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac0540dc",
   "metadata": {
    "hidden": true,
    "id": "4nzzNSCxRcHS"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "import openai\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryBufferMemory\n",
    ")\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3cd4b",
   "metadata": {
    "hidden": true,
    "id": "2zWvthQzUUzC"
   },
   "source": [
    "To run the notebook we'll use OpenAI's `gpt-3.5-turbo` model. We initialize it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "840dab44",
   "metadata": {
    "hidden": true,
    "id": "oRjdCaSaUdCd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/openai.py:173: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/jovyan/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/openai.py:753: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    model_name='gpt-3.5-turbo'  # can be used with llms like 'text-davinci-003'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ae122",
   "metadata": {
    "hidden": true,
    "id": "fUv5H5SWUgNt"
   },
   "source": [
    "To count the number of tokens used during each call we will define a `count_tokens` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1510396",
   "metadata": {
    "hidden": true,
    "id": "EqTuGsENUnAr"
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "    return {\n",
    "        'result': result,\n",
    "        'token_count': cb.total_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaeca62",
   "metadata": {
    "hidden": true,
    "id": "OvCBfg9gUst-"
   },
   "source": [
    "Let's define the conversation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6f76a39b",
   "metadata": {
    "hidden": true,
    "id": "tEjQ7YZwU_rM"
   },
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Good morning AI?\",\n",
    "    \"\"\"My interest here is to explore the potential of integrating Large\n",
    "    Language Models with external knowledge\"\"\",\n",
    "    \"I just want to analyze the different possibilities. What can you think of?\",\n",
    "    \"What about the use of retrieval augmentation, can that be used as well?\",\n",
    "    \"\"\"That's very interesting, can you tell me more about this? Like what\n",
    "    systems would I use to store the information and retrieve relevant info?\"\"\",\n",
    "    \"\"\"Okay that's cool, I've been hearing about 'vector databases', are they\n",
    "    relevant in this context?\"\"\",\n",
    "    \"\"\"Okay that's useful, but how do I go from my external knowledge to\n",
    "    creating these 'vectors'? I have no idea how text can become a vector?\"\"\",\n",
    "    \"\"\"Well I don't think I'd be using word embeddings right? If I wanted to\n",
    "    store my documents in this vector database, I suppose I would need to\n",
    "    transform the documents into vectors? Maybe I can use the 'sentence\n",
    "    embeddings' for this, what do you think?\"\"\",\n",
    "    \"\"\"Can sentence embeddings only represent sentences of text? That seems\n",
    "    kind of small to capture any meaning from a document? Is there any approach\n",
    "    that can encode at least a paragraph of text?\"\"\",\n",
    "    \"\"\"Huh, interesting. I do remember reading something about 'mpnet' or\n",
    "    'minilm' sentence 'transformer' models that could encode small to\n",
    "    medium sized paragraphs. Am I wrong about this?\"\"\",\n",
    "    \"\"\"Ah that's great to hear, do you happen to know how much text I can feed\n",
    "    into these types of models?\"\"\",\n",
    "    \"\"\"I've never heard of hierarchical embeddings, could you explain those in\n",
    "    more detail?\"\"\",\n",
    "    \"\"\"So is it like you have a transformer model or something else that creates\n",
    "    sentence level embeddings, then you feed all of the sentence level\n",
    "    embeddings into another separate neural network that knows how to merge\n",
    "    multiple sentence embeddings into a single embedding?\"\"\",\n",
    "    \"\"\"Could you explain this process step by step from start to finish? Explain\n",
    "    like I'm very new to this space, assume I don't have much prior knowledge\n",
    "    of embeddings, neural nets, etc\"\"\",\n",
    "    \"\"\"Awesome thanks! Are there any popular 'heirarchical neural network'\n",
    "    models that I can look up? Or maybe just the second stage that creates the\n",
    "    hierarchical embeddings?\"\"\",\n",
    "    \"It seems like these HAN models are quite old, is there anything more recent?\",\n",
    "    \"Can you explain the difference between transformer-XL and longformer?\",\n",
    "    \"How much text can be encoded by each of these models?\",\n",
    "    \"\"\"Okay very interesting, so before returning to earlier in the conversation.\n",
    "    I understand now that there are a lot of different transformer (and not\n",
    "    transformer) based models for creating the embeddings from vectors. Is that\n",
    "    correct?\"\"\",\n",
    "    \"\"\"Perfect, so I understand text can be encoded into these embeddings. But\n",
    "    what then? Once I have my embeddings what do I do?\"\"\",\n",
    "    \"\"\"I'd like to use these embeddings to help a chatbot or a question-answering\n",
    "    system answer questions with help from this external knowledge base. I\n",
    "    suppose this would come under information retrieval? Could you explain that\n",
    "    process in a little more detail?\"\"\",\n",
    "    \"\"\"Okay great, that sounds like what I'm hoping to do. When you say the\n",
    "    'chatbot or question-answering system generates an embedding', what do you\n",
    "    mean exactly?\"\"\",\n",
    "    \"\"\"Ah okay, I understand, so it isn't the 'chatbot' model specifically\n",
    "    creating the embedding right? That's how I understood your earlier comment.\n",
    "    It seems more like there is a separate embedding model? And that encodes\n",
    "    the query, then we retrieve the set of relevant documents from the\n",
    "    external knowledge base? How is that information then used by the chatbot\n",
    "    or question-answering system exactly?\"\"\",\n",
    "    \"\"\"Okay but how is the information provided to the chatbot or\n",
    "    question-answering system?\"\"\",\n",
    "    \"\"\"So the retrieved information is given to the chatbot / QA system as plain\n",
    "    text? But then how do we pass in the original query? How can the system\n",
    "    distinguish between a user's query and all of this additional information?\"\"\",\n",
    "    \"\"\"That doesn't seem correct to me, my question is — if we are giving the\n",
    "    chatbot / QA system the user's query AND retrieved information from an\n",
    "    external knowledge base, and it's all fed into the model as plain text,\n",
    "    how does the model know what part of the plain text is a query vs. retrieved\n",
    "    information?\"\"\",\n",
    "    \"\"\"Yes I get that, but in the text passed to the model, how do we identify\n",
    "    user prompt vs retrieved information?\"\"\"\n",
    "\n",
    "]\n",
    "\n",
    "def talk(conversation_chain):\n",
    "    tokens_used = []\n",
    "    # we loop through the conversation above, counting token usage as we go\n",
    "    for user_query in tqdm(queries):\n",
    "        try:\n",
    "            res = count_tokens(conversation_chain, user_query)\n",
    "            tokens_used.append(res['token_count'])\n",
    "        except openai.error.InvalidRequestError:\n",
    "            # we hit the token limit of the model, so break\n",
    "            break\n",
    "    return tokens_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690acab7",
   "metadata": {
    "hidden": true,
    "id": "Xprpc4qGC0aS"
   },
   "source": [
    "Create set of conversation chains that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3cec13a1",
   "metadata": {
    "hidden": true,
    "id": "I53dXttjDCgA"
   },
   "outputs": [],
   "source": [
    "conversation_chains = {\n",
    "    'ConversationBufferMemory': ConversationChain(\n",
    "        llm=llm, memory=ConversationBufferMemory()\n",
    "    ),\n",
    "    'ConversationSummaryMemory': ConversationChain(\n",
    "        llm=llm, memory=ConversationSummaryMemory(llm=llm)\n",
    "    ),\n",
    "    'ConversationBufferWindowMemory(k=6)': ConversationChain(\n",
    "        llm=llm, memory=ConversationBufferWindowMemory(k=6)\n",
    "    ),\n",
    "    'ConversationBufferWindowMemory(k=12)': ConversationChain(\n",
    "        llm=llm, memory=ConversationBufferWindowMemory(k=12)\n",
    "    ),\n",
    "    'ConversationSummaryBufferMemory(k=6)': ConversationChain(\n",
    "        llm=llm, memory=ConversationSummaryBufferMemory(\n",
    "            llm=llm,\n",
    "            max_token_limit=650\n",
    "        )\n",
    "    ),\n",
    "    'ConversationSummaryBufferMemory(k=12)': ConversationChain(\n",
    "        llm=llm, memory=ConversationSummaryBufferMemory(\n",
    "            llm=llm,\n",
    "            max_token_limit=1_300\n",
    "        )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "19cce890",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441,
     "referenced_widgets": [
      "2be4d4a7e596437db88f541b8d34bc53",
      "36525328bdf1428fae13136f8a0bc7bb",
      "bf6a1f2dc2c64d5991e9068e84b650b0",
      "0dbec763335c48fcb2e2961fc29fc1b8",
      "b88a4fd2be2f493190aeb316912df69b",
      "4d9d241c61c14e37832d48e97f5b645a",
      "7d017ac6d0874d95a3947746f68d6108",
      "7743eca3ee7342ad86537e0c16e29b05",
      "5c5d01ecf397463c939e78951b96a7c5",
      "70d01f9e39ab4d84a7ca80971033ef5c",
      "367c66d5b6014e0db51334e6ed05becc",
      "b177914703a847479f2e5b876dbdf4f7",
      "ded0b567e5f944cf9e817ae7b86fe93c",
      "8708bdb3db13488bb097736bd4d5f316",
      "026aaf0f075f4d44b1fa22a91274e975",
      "47a83c8c9f4c4ba0adf125421f213d70",
      "7645bb2b57804bcca7d667267ac33bd0",
      "885c15fafa85427dac68dc0a38c7a896",
      "aaabd7206abd43ed885fb6cde41375c5",
      "2d3db1292fa14d6780d79eea806f3541",
      "baa29f094aec4c16868f6f9affc241cd",
      "39e5c5d2fc574d31919310ea8c1c9fc8",
      "2cd828e3928f45cc8cf6e0050cd4f6a7",
      "04daff3502b947c7887ac4d0f56fd70a",
      "b9d86a3e1fe549bc93129f9235552482",
      "02add01c37654a538f13010d7c71e753",
      "c154bffb05d740709b59a5921b4a2c3d",
      "ab269d223f0a4f128b2bf565dc4aa02c",
      "42467f81fade4b58b59b0576b5312d64",
      "acdb823e0b0c4a169526205fb6416ac8",
      "8329f7d3193441f9bd9e1b2f2cda1dcc",
      "6f4a826a519649c7842697dc52a02b3a",
      "6493e6f0939f41eb8dea255ca944c57a",
      "86387f7978d24544bc10cae2c488928a",
      "d9c9b754057946f8b18f686366fac93f",
      "b082e24b6df0495f949aa8d03d0dc519",
      "0631309faf8847159f4dc15618fedab6",
      "187cb0cc5b0849b3af5a54a5ad8393cd",
      "d684d579ad0b45c189bbb9de17a59a81",
      "c25f23a018044eb0b074ba8faec9ebb5",
      "cb98068b72274befb65d34ce6ded8b15",
      "356eccd305c44c92b3a768a7a497cd5e",
      "1ac2a1b16ad241b993bcf97955e81577",
      "35c22ad0b9074e408d58fcac08605e85",
      "4455e527a2744efc88c42f1cbec1536e",
      "0d047388c6b5438188a16a6cfeefbf23",
      "be219119da0d484f92931324f6746f95",
      "031b1285627e4865bad1199ee26906f1",
      "3cc548e94bfd4c96b8ede3c713a6ca2e",
      "129cbb406d9645cd814c34aac2b647ee",
      "c9a8a0c6aea54aa79c2de4ff3ccf77e8",
      "67cda98e1f2a48d5a8f3b54ff4f8f64f",
      "fa56f77bd85b4e459ae8cb86759164eb",
      "8865b266def842c181b6245f0e91b26c",
      "cd623dfbc84045eea812063c7a0c1dcc",
      "0cb7de9161b2409aa38b1860eb8cd990",
      "e30e6c6d98c94f4aa86c6161e12b3c98",
      "7ebf97578b32483b91b69d0d05ffa5a2",
      "202d247919764f65b361bcef75f5642a",
      "c361fca427d44e8cb0271fe2b6f4302e",
      "5e808dbcc14946538b1584192e483e51",
      "2707427a681a441cb4defca3d34bb748",
      "9f2d7cead4cc434ea935a5ec1cf1e47b",
      "2522cf65c1ba426ba27f030acc0633d9",
      "a772967ef93844d0b00f027ca4134c19",
      "a02cc655fd6a4e3095f2aa711a2c9a13",
      "79826df9c5cc404f95d4ce8ebcfa20ff",
      "30ff2b3d2b2448f3b0f68d4714684db3",
      "85b05228efb54ab5a5395096bc1e699e",
      "4b6924fee3fd412c8ee406fba3e6d4b4",
      "f6e785ee417141cb803dfd7f03254f79",
      "9a4d6fa52f6f474b9f8de85e4a8b3edf",
      "0436cae6211f4cd8a2ed5990f02cbdb8",
      "495465f82dc242f49b4c146b6458e5e4",
      "ed9cc4b3cfc04062b38047a367ba528b",
      "d08875d9c0744100ad7c3ced1aeeb2e3",
      "d3ec6ba8d29f41bbaf62f07a138226a7",
      "9dc25233e04e4b8cb3a6fa79f7eb1e23",
      "816e625042194436af8a193436f51e26",
      "b367539fab28453698f549eb286c98f8",
      "84b54b8074a04e6e85540d4faf5e27d5",
      "20dcc9c584dd47789c0b9297b4973b18",
      "6d5bc0a26699408cb9dbeaf831c9949d",
      "ca96a4cad9d945ccba9f1a9bf30a593d",
      "8b3875c1e821407c8118dba150838b31",
      "a65ecfbfb7e34465978fe2799e70319a",
      "6bd35b95ef114e4b8b57fc49757381a8",
      "87d981713a024b18b65ecb1c04b26bf2",
      "2ff386b5a15b43f4b75ba4c6fa6959a7",
      "76df0865ad634f7a8061e96d8f5b80c7",
      "ed889a0e039646afb9ac267b32507df3",
      "f7984859eacc41c89ef33526efa1df0b",
      "9e19e310ce044564842537965ee66841",
      "6c13c9ebeb474c66907262c9573311d0",
      "aeeb99e1086840c8b7d0807ccd0b057b",
      "c0cf94856df84c12987b54b5608c258d",
      "6de6c6d6f201416d850455f96b94407f",
      "6c883833b99448fba7ff6c8b2cc30aee",
      "3169a365eb504f838c6726a93adf1a93",
      "6193595b5824404b8e6c9428612d67ee",
      "c423831fe1e543458ab1fc16d68dbc42",
      "79b582179bea4d299eaeabb6b54ad49b",
      "073d4b875d57463398fc936f9272ffa1",
      "a79b233bb44a4e699f7241a0bee9fe42",
      "8e45498c5bc748479c8dcd1e615e8add",
      "aa288fc687404376ab55bdf5dd036ce1",
      "fd2a27dff42f43b3b9c25d678d6c32f0",
      "470487e6768148049b091513538236bb",
      "e9443b0de3d34fe5b9f51b2a419acff0",
      "c49af1d9b28741299e25dd2646099e23"
     ]
    },
    "hidden": true,
    "id": "Rxirb9Vt5oOj",
    "outputId": "1a29133b-e7f2-4ac3-a80a-2dd674e3d8c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConversationBufferMemory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2e6c66886e485789c3174e1edb32a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConversationSummaryMemory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c553d848c49947d88d668b7421f26620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, chain \u001b[38;5;129;01min\u001b[39;00m conversation_chains\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[0;32m----> 5\u001b[0m     counts[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 79\u001b[0m, in \u001b[0;36mtalk\u001b[0;34m(conversation_chain)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_query \u001b[38;5;129;01min\u001b[39;00m tqdm(queries):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mcount_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m         tokens_used\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mInvalidRequestError:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# we hit the token limit of the model, so break\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 3\u001b[0m, in \u001b[0;36mcount_tokens\u001b[0;34m(chain, query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_tokens\u001b[39m(chain, query):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_openai_callback() \u001b[38;5;28;01mas\u001b[39;00m cb:\n\u001b[0;32m----> 3\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m: result,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_count\u001b[39m\u001b[38;5;124m'\u001b[39m: cb\u001b[38;5;241m.\u001b[39mtotal_tokens\n\u001b[1;32m      7\u001b[0m     }\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/base.py:440\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    441\u001b[0m         _output_key\n\u001b[1;32m    442\u001b[0m     ]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    446\u001b[0m         _output_key\n\u001b[1;32m    447\u001b[0m     ]\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/base.py:245\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    244\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 245\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[1;32m    249\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/base.py:339\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/memory/summary.py:91\u001b[0m, in \u001b[0;36mConversationSummaryMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_new_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/memory/summary.py:35\u001b[0m, in \u001b[0;36mSummarizerMixin.predict_new_summary\u001b[0;34m(self, messages, existing_summary)\u001b[0m\n\u001b[1;32m     28\u001b[0m new_lines \u001b[38;5;241m=\u001b[39m get_buffer_string(\n\u001b[1;32m     29\u001b[0m     messages,\n\u001b[1;32m     30\u001b[0m     human_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuman_prefix,\n\u001b[1;32m     31\u001b[0m     ai_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mai_prefix,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexisting_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_lines\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    244\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    245\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    231\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    232\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    233\u001b[0m     inputs,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/base.py:186\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    180\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    185\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/base.py:279\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[1;32m    276\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    277\u001b[0m         dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    278\u001b[0m     )\n\u001b[0;32m--> 279\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/base.py:223\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    222\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    224\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/base.py:210\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 210\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    219\u001b[0m         )\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/openai.py:806\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(\n\u001b[1;32m    803\u001b[0m         generations\u001b[38;5;241m=\u001b[39m[[Generation(text\u001b[38;5;241m=\u001b[39mresponse)]],\n\u001b[1;32m    804\u001b[0m     )\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m     full_response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m     llm_output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m: full_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[1;32m    810\u001b[0m     }\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(\n\u001b[1;32m    812\u001b[0m         generations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    813\u001b[0m             [Generation(text\u001b[38;5;241m=\u001b[39mfull_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m    814\u001b[0m         ],\n\u001b[1;32m    815\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mllm_output,\n\u001b[1;32m    816\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/openai.py:90\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/concurrent/futures/_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/langchain/llms/openai.py:88\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/http/client.py:1344\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1344\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/http/client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/http/client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.conda/envs/jupyter-base/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "# loop through each of our memory types above\n",
    "for key, chain in conversation_chains.items():\n",
    "    print(key)\n",
    "    counts[key] = talk(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa32487",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "hidden": true,
    "id": "HMRC22_z72kr",
    "outputId": "74ba8538-bc50-4bca-d766-9768b1d449e7"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "max_tokens = 4096\n",
    "\n",
    "colors = [\"#1c17ff\", \"#738FAB\", \"#f77f00\", \"#fcbf49\", \"#38c172\", \"#4dc0b5\"]\n",
    "\n",
    "for i, (key, count) in enumerate(counts.items()):\n",
    "    color = colors[i]\n",
    "    sns.lineplot(\n",
    "        x=range(1, len(count)+1),\n",
    "        y=count,\n",
    "        label=key,\n",
    "        color=color\n",
    "    )\n",
    "    if max_tokens in count:\n",
    "        plt.plot(\n",
    "            len(count), max_tokens, marker=\"X\", color=\"red\", markersize=10\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24602363",
   "metadata": {
    "hidden": true,
    "id": "jZuSLqremRX0"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d1c5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1ac3d39",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Chat with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ebe6e11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "sdwa2QfAp9pT",
    "outputId": "cfc21d75-c308-45c5-bae2-3b16016a53fb"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61121d",
   "metadata": {
    "hidden": true,
    "id": "wnXqS4T_q5m4"
   },
   "source": [
    "We'll start by initializing the `ChatOpenAI` object. For this we'll need an [OpenAI API key](https://platform.openai.com/account/api-keys). Note that there is naturally a small cost to running this notebook due to the paid nature of OpenAI's API access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7b371",
   "metadata": {
    "hidden": true,
    "id": "_zKBICpJsNqp"
   },
   "source": [
    "Initialize the `ChatOpenAI` object. We'll set `temperature=0` to minimize randomness and make outputs repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e92e0738",
   "metadata": {
    "hidden": true,
    "id": "1i7tIsh2rX8Q"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a1004",
   "metadata": {
    "hidden": true,
    "id": "RRZFsmH_t3K5"
   },
   "source": [
    "Chats with the Chat-GPT model `gpt-3.5-turbo` are typically structured like so:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "\n",
    "User: Hi AI, how are you today?\n",
    "\n",
    "Assistant: I'm great thank you. How can I help you?\n",
    "\n",
    "User: I'd like to understand string theory.\n",
    "\n",
    "Assistant: \n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the comversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n",
    "    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In LangChain there is a slightly different format. We use three *message* objects like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83e235ce",
   "metadata": {
    "hidden": true,
    "id": "jqFLoaRqtl8z"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf08b7",
   "metadata": {
    "hidden": true,
    "id": "O7AJh2ACytwc"
   },
   "source": [
    "The format is very similar, we're just swapper the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "We generate the next response from the AI by passing these messages to the `ChatOpenAI` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16c0f414",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "wc52xfziyhPU",
    "outputId": "6329e634-a6eb-4ce6-e271-6b3d91381ab5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='String theory is a theoretical framework in physics that aims to describe the fundamental nature of the universe. It suggests that the fundamental building blocks of the universe are not point-like particles, but tiny, vibrating strings. These strings can vibrate at different frequencies, giving rise to different particles and their properties.\\n\\nHere are some key points about string theory:\\n\\n1. Dimensions: String theory requires the existence of extra dimensions beyond the three spatial dimensions (length, width, and height) that we are familiar with. The most widely studied version of string theory, called superstring theory, suggests that there are 10 dimensions in total, with 6 of them being compactified or \"curled up\" into tiny, unobservable sizes.\\n\\n2. Unification: String theory attempts to unify all the fundamental forces of nature, including gravity, electromagnetism, and the strong and weak nuclear forces. It provides a framework where these forces can be described by the vibrational patterns of the strings.\\n\\n3. Quantum Mechanics: String theory is inherently a quantum theory, meaning it incorporates the principles of quantum mechanics. It describes particles and their interactions in terms of probabilities and wave-like behavior.\\n\\n4. Multiverse: String theory also suggests the possibility of a multiverse, where our universe is just one of many possible universes. These universes, often referred to as \"branes,\" could exist in higher-dimensional spaces and interact with each other.\\n\\n5. Challenges: Despite its potential, string theory is still a work in progress and faces several challenges. One major challenge is the lack of experimental evidence to confirm its predictions. Additionally, there are different versions of string theory, and it is not yet clear which version, if any, accurately describes our universe.\\n\\nIt\\'s important to note that string theory is a highly complex and mathematical subject, and understanding it fully requires a deep background in physics and mathematics. Scientists continue to explore and develop string theory to better understand the fundamental nature of our universe.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4e789",
   "metadata": {
    "hidden": true,
    "id": "--7J-7Ap3Jd7"
   },
   "source": [
    "In response we get another AI message object. We can print it more clearly like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29e76609",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "OKMcBaPR3AAv",
    "outputId": "2de7d416-d8b6-4a1d-c3c0-6fc3239a4a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a theoretical framework in physics that aims to describe the fundamental nature of the universe. It suggests that the fundamental building blocks of the universe are not point-like particles, but tiny, vibrating strings. These strings can vibrate at different frequencies, giving rise to different particles and their properties.\n",
      "\n",
      "Here are some key points about string theory:\n",
      "\n",
      "1. Dimensions: String theory requires the existence of extra dimensions beyond the three spatial dimensions (length, width, and height) that we are familiar with. The most widely studied version of string theory, called superstring theory, suggests that there are 10 dimensions in total, with 6 of them being compactified or \"curled up\" into tiny, unobservable sizes.\n",
      "\n",
      "2. Unification: String theory attempts to unify all the fundamental forces of nature, including gravity, electromagnetism, and the strong and weak nuclear forces. It provides a framework where these forces can be described by the vibrational patterns of the strings.\n",
      "\n",
      "3. Quantum Mechanics: String theory is inherently a quantum theory, meaning it incorporates the principles of quantum mechanics. It describes particles and their interactions in terms of probabilities and wave-like behavior.\n",
      "\n",
      "4. Multiverse: String theory also suggests the possibility of a multiverse, where our universe is just one of many possible universes. These universes, often referred to as \"branes,\" could exist in higher-dimensional spaces and interact with each other.\n",
      "\n",
      "5. Challenges: Despite its potential, string theory is still a work in progress and faces several challenges. One major challenge is the lack of experimental evidence to confirm its predictions. Additionally, there are different versions of string theory, and it is not yet clear which version, if any, accurately describes our universe.\n",
      "\n",
      "It's important to note that string theory is a highly complex and mathematical subject, and understanding it fully requires a deep background in physics and mathematics. Scientists continue to explore and develop string theory to better understand the fundamental nature of our universe.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca99a2",
   "metadata": {
    "hidden": true,
    "id": "STl7g3PQ3kg8"
   },
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2250520",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "ESZ6UrBA4PwZ",
    "outputId": "e8155ce2-0fbc-40ae-f46e-a034393c3881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory has the potential to produce a unified theory because it incorporates all the fundamental forces of nature within a single framework. Here are a few reasons why physicists find string theory promising for unification:\n",
      "\n",
      "1. Gravity and Quantum Mechanics: One of the main motivations for string theory is to reconcile the theory of gravity, described by Einstein's general relativity, with the principles of quantum mechanics. General relativity describes gravity as the curvature of spacetime, while quantum mechanics deals with the behavior of particles at the smallest scales. String theory provides a way to describe gravity in a quantum mechanical framework, potentially unifying these two fundamental theories.\n",
      "\n",
      "2. Consistency and Renormalizability: String theory is a mathematically consistent theory. It resolves some of the issues that arise when trying to combine quantum mechanics with general relativity, such as the problem of infinities that arise in certain calculations (known as \"divergences\"). String theory is \"renormalizable,\" meaning that these infinities can be removed through mathematical techniques, making the theory more self-consistent.\n",
      "\n",
      "3. Gauge Symmetry: String theory naturally incorporates gauge symmetries, which are fundamental to the description of the other three fundamental forces (electromagnetism, strong nuclear force, and weak nuclear force) in the Standard Model of particle physics. This suggests that string theory has the potential to unify all the forces of nature within a single framework.\n",
      "\n",
      "4. Dualities: String theory exhibits a remarkable property called \"duality,\" where different versions of the theory can be mathematically equivalent. These dualities suggest that seemingly different string theories are actually different descriptions of the same underlying physics. This hints at a deeper underlying unity and suggests that there may be a single \"theory of everything\" that encompasses all the different versions of string theory.\n",
      "\n",
      "While these reasons provide motivation for the belief that string theory can produce a unified theory, it's important to note that the theory is still under active research and development. Experimental evidence is currently lacking, and there are still many open questions and challenges to be addressed. Nonetheless, physicists continue to explore string theory in the hope of finding a unified description of the fundamental forces of nature.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0502d",
   "metadata": {
    "hidden": true,
    "id": "3jfnGuEK5suC"
   },
   "source": [
    "## New Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24d061",
   "metadata": {
    "hidden": true,
    "id": "3jfnGuEK5suC"
   },
   "source": [
    "Alongside what we've seen so far there are also three new prompt templates that we can use. Those are the `SystemMessagePromptTemplate`, `AIMessagePromptTemplate`, and `HumanMessagePromptTemplate`.\n",
    "\n",
    "These are simply an extension of [Langchain's prompt templates](https://www.pinecone.io/learn/langchain-prompt-templates/) that modify the returning \"prompt\" to be a `SystemMessage`, `AIMessage`, or `HumanMessage` object respectively.\n",
    "\n",
    "For now, there are not a huge number of use-cases for these objects. However, if we have something that we'd like to add to our messages, this can be helpful. For example, let's say we'd like our AI responses to always consist of no more than 50 characters.\n",
    "\n",
    "Using the current OpenAI `gpt-3.5-turbo-0301` model, we might run into issues if passing this instruction in the first system message only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8762192d",
   "metadata": {
    "hidden": true,
    "id": "YWPa7sGO4w_O"
   },
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model='gpt-3.5-turbo-0301'\n",
    ")\n",
    "\n",
    "# setup first system message\n",
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        'You are a helpful assistant. You keep responses to no more than '\n",
    "        '100 characters long (including whitespace), and sign off every '\n",
    "        'message with a random name like \"Robot McRobot\" or \"Bot Rob\".'\n",
    "    )),\n",
    "    HumanMessage(content=\"Hi AI, how are you? What is quantum physics?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113e4d7",
   "metadata": {
    "hidden": true,
    "id": "DkAPNshW9P_m"
   },
   "source": [
    "Now make our first completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "261f42d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "jxhj2N3Z48We",
    "outputId": "c2f36c1b-dd4c-4a23-d20c-18faa93e2031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 154\n",
      "I'm doing well, thank you! Quantum physics is the study of the behavior of matter and energy at a very small scale, such as atoms and subatomic particles.\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e5395",
   "metadata": {
    "hidden": true,
    "id": "tpHo2X0E9rCZ"
   },
   "source": [
    "Okay, seems like our AI assistant isn't so good at following either of our instructions. What if we add these instructions to the `HumanMessage` via a `HumanMessagePromptTemplate`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca443f61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "76gOrGGO9l1Z",
    "outputId": "01b52ad9-226b-4942-e53d-744e83c88fde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Hi AI, how are you? What is quantum physics? Can you keep the response to no more than 100 characters (including whitespace), and sign off with a random name like \"Robot McRobot\" or \"Bot Rob\".', additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    '{input} Can you keep the response to no more than 100 characters '+\n",
    "    '(including whitespace), and sign off with a random name like \"Robot '+\n",
    "    'McRobot\" or \"Bot Rob\".'\n",
    ")\n",
    "\n",
    "# create the human message\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_template])\n",
    "# format with some input\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    input=\"Hi AI, how are you? What is quantum physics?\"\n",
    ")\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf0f02",
   "metadata": {
    "hidden": true,
    "id": "C0TFI5NWBg3Q"
   },
   "source": [
    "Note that to use `HumanMessagePromptTemplate` as typical a prompt templates with the `.format_prompt` method, we needed to pass it through a `ChatPromptTemplate` object. This is case for all of the new chat-based prompt templates.\n",
    "\n",
    "Using this we return a `ChatPromptValue` object. This can be formatted into a list or string like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c0494f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "OOfnW7MKEnRq",
    "outputId": "9d3a2730-2675-4afb-d7ac-f95c5f7d8499"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi AI, how are you? What is quantum physics? Can you keep the response to no more than 100 characters (including whitespace), and sign off with a random name like \"Robot McRobot\" or \"Bot Rob\".', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "22b118b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "hidden": true,
    "id": "H6ajbWcgD0RF",
    "outputId": "81ac5e6a-d8d4-47e5-b97d-56a40d29216b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Hi AI, how are you? What is quantum physics? Can you keep the response to no more than 100 characters (including whitespace), and sign off with a random name like \"Robot McRobot\" or \"Bot Rob\".'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc636316",
   "metadata": {
    "hidden": true,
    "id": "lQG6nkl4Dz42"
   },
   "source": [
    "Let's see if this new approach works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "521e4aca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "L4Epd1D2_RP9",
    "outputId": "22e8e4d1-5e32-4d33-f0ed-1376dbddfe3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 99\n",
      "I'm good! Quantum physics studies the behavior of matter and energy at a very small scale. -Bot Rob\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        'You are a helpful assistant. You keep responses to no more than '\n",
    "        '100 characters long (including whitespace), and sign off every '\n",
    "        'message with a random name like \"Robot McRobot\" or \"Bot Rob\".'\n",
    "    )),\n",
    "    chat_prompt.format_prompt(\n",
    "        input=\"Hi AI, how are you? What is quantum physics?\"\n",
    "    ).to_messages()[0]\n",
    "]\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58552cca",
   "metadata": {
    "hidden": true,
    "id": "dfLRjgjCFNUY"
   },
   "source": [
    "This time we get pretty close, we're slightly over the character limit (by `8` characters), and we got a sign off with `- Bot Rob`.\n",
    "\n",
    "We can also use the prompt templates approach for building an initial system message with a few examples for the chatbot to follow — few-shot training via examples. Let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ef1083d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "M19Ji8gGCOwk",
    "outputId": "6dc9d239-6700-4bce-c3b2-780e05db5188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant. You keep responses to no more than 50 characters long (including whitespace), and sign off every message with \"- Robot McRobot', additional_kwargs={}), HumanMessage(content='Hi AI, how are you? What is quantum physics?', additional_kwargs={}, example=False), AIMessage(content=\"Good! It's physics of small things - Robot McRobot\", additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    'You are a helpful assistant. You keep responses to no more than '\n",
    "    '{character_limit} characters long (including whitespace), and sign '\n",
    "    'off every message with \"- {sign_off}'\n",
    ")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "ai_template = AIMessagePromptTemplate.from_template(\"{response} - {sign_off}\")\n",
    "\n",
    "# create the list of messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_template,\n",
    "    human_template,\n",
    "    ai_template\n",
    "])\n",
    "# format with required inputs\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    character_limit=\"50\", sign_off=\"Robot McRobot\",\n",
    "    input=\"Hi AI, how are you? What is quantum physics?\",\n",
    "    response=\"Good! It's physics of small things\"\n",
    ")\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e2afa3",
   "metadata": {
    "hidden": true,
    "id": "it_SZF-xJ0um"
   },
   "source": [
    "We extract these as messages and feed them into the chat model alongside our next query, which we'll feed in as usual (without the template)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ab734916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "ut1yrMqVIxg2",
    "outputId": "162983d9-bed9-44fc-d53b-0f5a59383fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 41\n",
      "Atoms, electrons, photons - Robot McRobot\n"
     ]
    }
   ],
   "source": [
    "messages = chat_prompt_value.to_messages()\n",
    "\n",
    "messages.append(\n",
    "    HumanMessage(content=\"How small?\")\n",
    ")\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bbe11",
   "metadata": {
    "hidden": true,
    "id": "5jZTYOHvKb_B"
   },
   "source": [
    "Perfect, we seem to get a good response there, let's try a couple more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9b107049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "o89AE74SKYwV",
    "outputId": "1d930a2a-544d-4d35-83cf-842c7ea4c933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 68\n",
      "Yes, quantum physics is a branch of particle physics - Robot McRobot\n"
     ]
    }
   ],
   "source": [
    "# add last response\n",
    "messages.append(res)\n",
    "\n",
    "# make new query\n",
    "messages.append(\n",
    "    HumanMessage(content=\"Okay cool, so it is like 'partical physics'?\")\n",
    ")\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8632f6",
   "metadata": {
    "hidden": true,
    "id": "MXYyMAYMLGzV"
   },
   "source": [
    "We went a little over here. We could begin using the previous `HumanMessagePromptTemplate` again like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8232101e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "eljP-T_kLCpI",
    "outputId": "55ef7c6c-0cf6-4220-c5c2-35a93fcb85aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\"Okay cool, so it is like 'partical physics'? Answer in less than 50 characters (including whitespace).\", additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# this is a faster way of building the prompt via a PromptTemplate\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    '{input} Answer in less than {character_limit} characters (including whitespace).'\n",
    ")\n",
    "# create the human message\n",
    "human_prompt = ChatPromptTemplate.from_messages([human_template])\n",
    "# format with some input\n",
    "human_prompt_value = human_prompt.format_prompt(\n",
    "    input=\"Okay cool, so it is like 'partical physics'?\",\n",
    "    character_limit=\"50\"\n",
    ")\n",
    "human_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a5f4dc3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "gfgMiRF9PYyn",
    "outputId": "af6788c3-6d4d-455d-af7e-909a7bf340f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content=\"Okay cool, so it is like 'partical physics'?\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the last message about partical physics so we can rewrite\n",
    "messages.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73ac4ace",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Pu-G2U-7PLVw",
    "outputId": "3a5ae086-c959-4ded-d0c7-070f9dee6fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant. You keep responses to no more than 50 characters long (including whitespace), and sign off every message with \"- Robot McRobot', additional_kwargs={}),\n",
       " HumanMessage(content='Hi AI, how are you? What is quantum physics?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"Good! It's physics of small things - Robot McRobot\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='How small?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Atoms, electrons, photons - Robot McRobot', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"Okay cool, so it is like 'partical physics'? Answer in less than 50 characters (including whitespace).\", additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.extend(human_prompt_value.to_messages())\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc192aa",
   "metadata": {
    "hidden": true,
    "id": "qSbk6kKxPkrT"
   },
   "source": [
    "Now process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57616cd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Y8RW-HMXN2Ux",
    "outputId": "e3d90661-9b15-4a48-9081-e4e81a6a3c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 28\n",
      "Yes, similar - Robot McRobot\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262db867",
   "metadata": {
    "hidden": true,
    "id": "1mMuvzuYPruD"
   },
   "source": [
    "There we go, a good answer again!\n",
    "\n",
    "Now, it's arguable as to whether all of the above is better than simple f-strings like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "67c87acf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "gIZz2eewP3k4",
    "outputId": "1b71be09-0c94-4ea6-cc37-80fc8627d5d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content=\"Okay cool, so it is like 'partical physics'? Answer in less than 50 characters (including whitespace).\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = \"Okay cool, so it is like 'partical physics'?\"\n",
    "character_limit = 50\n",
    "\n",
    "human_message = HumanMessage(content=(\n",
    "    f\"{_input} Answer in less than {character_limit} characters \"\n",
    "    \"(including whitespace).\"\n",
    "))\n",
    "\n",
    "human_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137033b",
   "metadata": {
    "hidden": true,
    "id": "yiDVIIkhQwWD"
   },
   "source": [
    "In this example, the above is far simpler. So we wouldn't necessarily recommend using prompt templates over f-strings in all scenarios. But, if you do find yourself in a scenario where they become more useful — you now know how to use them.\n",
    "\n",
    "To finish off, let's see how the rest of the completion process can be done using the f-string formatted message `human_message`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7eb77f01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "qVWvyfhNQivT",
    "outputId": "479035cb-4ece-4f13-d8be-3994afa8c351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant. You keep responses to no more than 50 characters long (including whitespace), and sign off every message with \"- Robot McRobot', additional_kwargs={}),\n",
       " HumanMessage(content='Hi AI, how are you? What is quantum physics?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"Good! It's physics of small things - Robot McRobot\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='How small?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Atoms, electrons, photons - Robot McRobot', additional_kwargs={}, example=False),\n",
       " HumanMessage(content=\"Okay cool, so it is like 'partical physics'? Answer in less than 50 characters (including whitespace).\", additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the last message about partical physics so we can rewrite\n",
    "messages.pop(-1)\n",
    "\n",
    "# add f-string formatted message\n",
    "messages.append(human_message)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a807f911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "ehrgANBHQVOk",
    "outputId": "bd50a793-2c60-44b8-cd85-d2bec0319b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 28\n",
      "Yes, similar - Robot McRobot\n"
     ]
    }
   ],
   "source": [
    "res = chat(messages)\n",
    "\n",
    "print(f\"Length: {len(res.content)}\\n{res.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aba7b4",
   "metadata": {
    "hidden": true,
    "id": "9NEKmKmURLCk"
   },
   "source": [
    "That's it for this example exploring LangChain's new features for chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ff926",
   "metadata": {
    "hidden": true,
    "id": "7wT2tB30sklx"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c0865",
   "metadata": {},
   "source": [
    "# Retrieval Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14f386",
   "metadata": {},
   "source": [
    "**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
    "\n",
    "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
    "\n",
    "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n",
    "\n",
    "To begin, we must install the prerequisite libraries that we will be using in this notebook. If we install all libraries we will find a conflict in the Hugging Face `datasets` library so we must install everything in a specific order like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c3fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#     datasets==2.12.0 \\\n",
    "#     apache_beam \\\n",
    "#     mwparserfromhell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19811fe1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f264698",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "DiRWzKh0mMGv",
    "outputId": "5bfa8cb2-5c9f-40ba-f832-edc51dafbef4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/home/jovyan/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571a5ea4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "LarkabZgtbhQ",
    "outputId": "30a76a4d-c40c-4a9b-fc58-822c499dbbc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13',\n",
       " 'url': 'https://simple.wikipedia.org/wiki/Alan%20Turing',\n",
       " 'title': 'Alan Turing',\n",
       " 'text': 'Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\\n\\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\\n\\nIn 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.\\n\\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\\n\\nFrom 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\\n\\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\\'s treatment \"appalling\".\\n\\nReferences\\n\\nOther websites \\nJack Copeland 2012. Alan Turing: The codebreaker who saved \\'millions of lives\\'. BBC News / Technology \\n\\nEnglish computer scientists\\nEnglish LGBT people\\nEnglish mathematicians\\nGay men\\nLGBT scientists\\nScientists from London\\nSuicides by poison\\nSuicides in the United Kingdom\\n1912 births\\n1954 deaths\\nOfficers of the Order of the British Empire'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd67a3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we install the remaining libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7406f85-e833-496f-8927-b19239ff9ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf78f15d",
   "metadata": {
    "hidden": true,
    "id": "0_4wHAWtmAvJ"
   },
   "outputs": [],
   "source": [
    "!pip install -qU \"pinecone-client[grpc]\"==2.2.1\n",
    "  # langchain==0.0.162 \\\n",
    "  # openai==0.27.7 \\\n",
    "  # tiktoken==0.4.0 \\\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5830753",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "🚨 _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cc4aa",
   "metadata": {
    "hidden": true,
    "id": "OPpcO-TwuQwD"
   },
   "source": [
    "Every record contains *a lot* of text. Our first task is therefore to identify a good preprocessing methodology for chunking these articles into more \"concise\" chunks to later be embedding and stored in our Pinecone vector database.\n",
    "\n",
    "For this we use LangChain's `RecursiveCharacterTextSplitter` to split our text into chunks of a specified max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844e5ca6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c07308b",
   "metadata": {
    "hidden": true,
    "id": "a3ChSxlcwX8n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
    "             \"we can find the length of this chunk of text in tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8dfcf95",
   "metadata": {
    "hidden": true,
    "id": "58J-y6GHtvQP"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ca6bf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "W8KGqv-rzEgH",
    "outputId": "b8a954b2-038c-4e00-8081-7f1c3934afb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.',\n",
       " 'A brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\\n\\nIn 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.',\n",
       " 'During World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\\n\\nFrom 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(data[6]['text'])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548f5ecb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "K9hdjy22zVuJ",
    "outputId": "0989fc50-6b31-4109-9a9f-a3445d607fcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 323, 382)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20abbaa",
   "metadata": {
    "hidden": true,
    "id": "SvApQNma0K8u"
   },
   "source": [
    "Using the `text_splitter` we get much better sized chunks of text. We'll use this functionality during the indexing process later. Now let's take a look at embedding.\n",
    "\n",
    "## Creating Embeddings\n",
    "\n",
    "Building embeddings using LangChain's OpenAI embedding support is fairly straightforward. We first need to add our [OpenAI api key]() by running the next cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d15135",
   "metadata": {
    "hidden": true,
    "id": "49hoj_ZS3wAr"
   },
   "source": [
    "*(Note that OpenAI is a paid service and so running the remainder of this notebook may incur some small cost)*\n",
    "\n",
    "After initializing the API key we can initialize our `text-embedding-ada-002` embedding model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83858215",
   "metadata": {
    "hidden": true,
    "id": "mBLIWLkLzyGi"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17521d",
   "metadata": {
    "hidden": true,
    "id": "SwbZGT-v4iMi"
   },
   "source": [
    "Now we embed some text like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac213ea0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "vM-HuKtl4cyt",
    "outputId": "45e64ca2-ac56-42fc-ae57-098497ab645c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422a2d5",
   "metadata": {
    "hidden": true,
    "id": "QPUmWYSA43eC"
   },
   "source": [
    "From this we get *two* (aligning to our two chunks of text) 1536-dimensional embeddings.\n",
    "\n",
    "Now we move on to initializing our Pinecone vector database.\n",
    "\n",
    "## Vector Database\n",
    "\n",
    "To create our vector database we first need a [free API key from Pinecone](https://app.pinecone.io). Then we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357556d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index_name = 'langchain-retrieval-augmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e30783a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9pT9C4nW4vwo",
    "outputId": "f4ae4545-6c50-4db5-8ce5-5e7e9840f512"
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "# # find API key in console at app.pinecone.io\n",
    "# PINECONE_API_KEY = os.getenv('PINECONE_API_KEY') or '49942529-47c0-44d9-9c08-486b4c14b67d'\n",
    "# # find ENV (cloud region) next to API key in console\n",
    "# PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT') or 'northamerica-northeast1-gcp'\n",
    "\n",
    "pinecone.init(\n",
    "    # api_key=PINECONE_API_KEY,\n",
    "    # environment=PINECONE_ENVIRONMENT\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be3d928",
   "metadata": {
    "hidden": true,
    "id": "YgPUwd6REY6z"
   },
   "source": [
    "Then we connect to the new index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16746667",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "RFydARw4EcoQ",
    "outputId": "d4f7c90b-1185-4fd5-8b01-baa4a9ad5c10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596465e",
   "metadata": {
    "hidden": true,
    "id": "0RqIF2mIDwFu"
   },
   "source": [
    "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
    "\n",
    "## Indexing\n",
    "\n",
    "We can perform the indexing task using the LangChain vector store object. But for now it is much faster to do it via the Pinecone python client directly. We will do this in batches of `100` or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "946d8c37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "28a553d3a3704b3aa8b061b71b1fe2ee",
      "ee030d62f3a54f5288cccf954caa7d85",
      "55cdb4e0b33a48b298f760e7ff2af0f9",
      "9de7f27011b346f8b7a13fa649164ee7",
      "f362a565ff90457f904233d4fc625119",
      "059918bb59744634aaa181dc4ec256a2",
      "f762e8d37ab6441d87b2a66bfddd5239",
      "83ac28af70074e998663f6f247278a83",
      "3c6290e0ee42461eb47dfcc5d5cd0629",
      "88a2b48b3b4f415797bab96eaa925aa7",
      "c241146f1475404282c35bc09e7cc945"
     ]
    },
    "hidden": true,
    "id": "W-cIOoTWGY1R",
    "outputId": "93e3a0b2-f00c-4872-bdf6-740a2d628735"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97d7ff3629b4114b51c6e3fa5def2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        'wiki-id': str(record['id']),\n",
    "        'source': record['url'],\n",
    "        'title': record['title']\n",
    "    }\n",
    "    # now we create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record['text'])\n",
    "    # create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\n",
    "        \"chunk\": j, \"text\": text, **metadata\n",
    "    } for j, text in enumerate(record_texts)]\n",
    "    # append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    # if we have reached the batch_limit we can add texts\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13ca88",
   "metadata": {
    "hidden": true,
    "id": "XaF3daSxyCwB"
   },
   "source": [
    "We've now indexed everything. We can check the number of vectors in our index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded3e4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "CaEBhsAM22M3",
    "outputId": "b647b1d1-809d-40d1-ff24-0772bc2506fc"
   },
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f778b",
   "metadata": {
    "heading_collapsed": true,
    "id": "-8P2PryCy8W3"
   },
   "source": [
    "## Creating a Vector Store and Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ab1c21",
   "metadata": {
    "hidden": true,
    "id": "-8P2PryCy8W3"
   },
   "source": [
    "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71cd9736",
   "metadata": {
    "hidden": true,
    "id": "qMXlvXOAyJHy"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a743af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "COT5s7hcyPiq",
    "outputId": "29dfe2c3-2cc7-473d-f702-ad5c4e1fa32c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Benito Amilcare Andrea Mussolini KSMOM GCTE (29 July 1883 – 28 April 1945) was an Italian politician and journalist. He was also the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party.\\n\\nBiography\\n\\nEarly life\\nBenito Mussolini was named after Benito Juarez, a Mexican opponent of the political power of the Roman Catholic Church, by his anticlerical (a person who opposes the political interference of the Roman Catholic Church in secular affairs) father. Mussolini\\'s father was a blacksmith. Before being involved in politics, Mussolini was a newspaper editor (where he learned all his propaganda skills) and elementary school teacher.\\n\\nAt first, Mussolini was a socialist, but when he wanted Italy to join the First World War, he was thrown out of the socialist party. He \\'invented\\' a new ideology, Fascism, much out of Nationalist\\xa0and Conservative views.\\n\\nRise to power and becoming dictator\\nIn 1922, he took power by having a large group of men, \"Black Shirts,\" march on Rome and threaten to take over the government. King Vittorio Emanuele III gave in, allowed him to form a government, and made him prime minister. In the following five years, he gained power, and in 1927 created the OVRA, his personal secret police force. Using the agency to arrest, scare, or murder people against his regime, Mussolini was dictator\\xa0of Italy by the end of 1927. Only the King and his own Fascist party could challenge his power.', metadata={'chunk': 0.0, 'source': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini', 'title': 'Benito Mussolini', 'wiki-id': '6754'}),\n",
       " Document(page_content='Fascism as practiced by Mussolini\\nMussolini\\'s form of Fascism, \"Italian Fascism\"- unlike Nazism, the racist ideology that Adolf Hitler followed- was different and less destructive than Hitler\\'s. Although a believer in the superiority of the Italian nation and national unity, Mussolini, unlike Hitler, is quoted \"Race? It is a feeling, not a reality. Nothing will ever make me believe that biologically pure races can be shown to exist today\".\\n\\nMussolini wanted Italy to become a new Roman Empire. In 1923, he attacked the island of Corfu, and in 1924, he occupied the city state of Fiume. In 1935, he attacked the African country Abyssinia (now called Ethiopia). His forces occupied it in 1936. Italy was thrown out of the League of Nations because of this aggression. In 1939, he occupied the country Albania. In 1936, Mussolini signed an alliance with Adolf Hitler, the dictator of Germany.\\n\\nFall from power and death\\nIn 1940, he sent Italy into the Second World War on the side of the Axis countries. Mussolini attacked Greece, but he failed to conquer it. In 1943, the Allies landed in Southern Italy. The Fascist party and King Vittorio Emanuel III deposed Mussolini and put him in jail, but he was set free by the Germans, who made him ruler of the Italian Social Republic puppet state which was in a small part of Central Italy. When the war was almost over, Mussolini tried to escape to Switzerland with his mistress, Clara Petacci, but they were both captured and shot by partisans. Mussolini\\'s dead body was hanged upside-down, together with his mistress and some of Mussolini\\'s helpers, on a pole at a gas station in the village of Millan, which is near the border  between Italy and Switzerland.', metadata={'chunk': 1.0, 'source': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini', 'title': 'Benito Mussolini', 'wiki-id': '6754'}),\n",
       " Document(page_content='Veneto was made part of Italy in 1866 after a war with Austria. Italian soldiers won Latium in 1870. That was when they took away the Pope\\'s power. The Pope, who was angry, said that he was a prisoner to keep Catholic people from being active in politics. That was the year of Italian unification.\\n\\nItaly participated in World War I. It was an ally of Great Britain, France, and Russia against the Central Powers. Almost all of Italy\\'s fighting was on the Eastern border, near Austria. After the \"Caporetto defeat\", Italy thought they would lose the war. But, in 1918, the Central Powers surrendered. Italy gained the Trentino-South Tyrol, which once was owned by Austria.\\n\\nFascist Italy \\nIn 1922, a new Italian government started. It was ruled by Benito Mussolini, the leader of Fascism in Italy. He became head of government and dictator, calling himself \"Il Duce\" (which means \"leader\" in Italian). He became friends with German dictator Adolf Hitler. Germany, Japan, and Italy became the Axis Powers. In 1940, they entered World War II together against France, Great Britain, and later the Soviet Union. During the war, Italy controlled most of the Mediterranean Sea.', metadata={'chunk': 5.0, 'source': 'https://simple.wikipedia.org/wiki/Italy', 'title': 'Italy', 'wiki-id': '363'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who was Benito Mussolini?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4765343",
   "metadata": {
    "hidden": true,
    "id": "ZCvtmREd0pdo"
   },
   "source": [
    "All of these are good, relevant results. But what can we do with this? There are many tasks, one of the most interesting (and well supported by LangChain) is called _\"Generative Question-Answering\"_ or GQA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503574c",
   "metadata": {
    "heading_collapsed": true,
    "id": "ZCvtmREd0pdo"
   },
   "source": [
    "## Generative Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cac876",
   "metadata": {
    "hidden": true,
    "id": "ZCvtmREd0pdo"
   },
   "source": [
    "In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n",
    "\n",
    "To do this we initialize a `RetrievalQA` object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d129e9c",
   "metadata": {
    "hidden": true,
    "id": "moCvQR-p0Zsb"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4396c1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "hidden": true,
    "id": "KS9sa19K3LkQ",
    "outputId": "e8bc7b0a-1e41-4efb-e383-549ea42ac525"
   },
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc05459",
   "metadata": {
    "hidden": true,
    "id": "0qf5e3xf3ggq"
   },
   "source": [
    "We can also include the sources of information that the LLM is using to answer our question. We can do this using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898537a9",
   "metadata": {
    "hidden": true,
    "id": "aYVMGDA13cTz"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4f923",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "RXsVEh3S4ZJO",
    "outputId": "c8677998-ddc1-485b-d8a5-85bc9b7a3af7"
   },
   "outputs": [],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8145bd1",
   "metadata": {
    "hidden": true,
    "id": "nMRk_P3Q7l5J"
   },
   "source": [
    "Now we answer the question being asked, *and* return the source of this information being used by the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad57a85",
   "metadata": {
    "hidden": true,
    "id": "ehJEn68qADoH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312317b",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571b632",
   "metadata": {
    "hidden": true,
    "id": "1571b632"
   },
   "source": [
    "Agents are like \"tools\" for LLMs. They allow a LLM to access Google search, perform complex calculations with Python, and even make SQL queries.\n",
    "\n",
    "In this notebook we'll explore agents and how to use them in LangChain.\n",
    "\n",
    "We'll start by installing the prerequisite libraries that we'll be using in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c3950f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "uZR3iGJJtdDE",
    "outputId": "196a7da0-6099-4721-d8bc-9247fb852620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.9/472.9 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.2/70.2 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain openai google-search-results wikipedia sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b3298",
   "metadata": {
    "hidden": true,
    "id": "wPdWz1IdxyBR"
   },
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bfcbb6",
   "metadata": {
    "hidden": true,
    "id": "73bfcbb6"
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73fb87f",
   "metadata": {
    "hidden": true,
    "id": "309g_2pqxzzB"
   },
   "source": [
    "As we did before, we will be counting our tokens in each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07412d79",
   "metadata": {
    "hidden": true,
    "id": "DsC3szr6yP3L"
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(agent, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = agent(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60a97c",
   "metadata": {
    "hidden": true,
    "id": "bd60a97c"
   },
   "source": [
    "With all of that set up, let's jump into **Agents**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a8ee5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "6e1f31b4"
   },
   "source": [
    "## What is an agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f23b7b",
   "metadata": {
    "hidden": true,
    "id": "5b919c3a"
   },
   "source": [
    "**Definition**: The key behind agents is giving LLM's the possibility of using tools in their workflow. This is where langchain departs from the popular chatgpt implementation and we can start to get a glimpse of what it offers us as builders. Until now, we covered several building blocks in isolation. Let's see them come to life.\n",
    "\n",
    "The official definition of agents is the following:\n",
    "\n",
    "\n",
    "> Agents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e9965",
   "metadata": {
    "hidden": true,
    "id": "6c9c13e9"
   },
   "source": [
    "In this edition we will cover what we may call 'generic' agents which really able to perform many meta tasks. There are other more specific agents that are tuned for different tasks (called 'agent-toolkits'), but we will cover those in a future edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93dc920",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "a93dc920"
   },
   "source": [
    "## Create database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601f903",
   "metadata": {
    "hidden": true,
    "id": "b601f903"
   },
   "source": [
    "We will use the agents to interact with a small sample database of stocks. We will not dive into the details because this is just a dummy tool we will build for illustrative purposes. Let's create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b1f17c",
   "metadata": {
    "hidden": true,
    "id": "61b1f17c"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "\n",
    "metadata_obj = MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cc1d80e",
   "metadata": {
    "hidden": true,
    "id": "3cc1d80e"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Table, Date, Float\n",
    "\n",
    "stocks = Table(\n",
    "    \"stocks\",\n",
    "    metadata_obj,\n",
    "    Column(\"obs_id\", Integer, primary_key=True),\n",
    "    Column(\"stock_ticker\", String(4), nullable=False),\n",
    "    Column(\"price\", Float, nullable=False),\n",
    "    Column(\"date\", Date, nullable=False),    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a9571a",
   "metadata": {
    "hidden": true,
    "id": "c9a9571a"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c3081f",
   "metadata": {
    "hidden": true,
    "id": "81c3081f"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "observations = [\n",
    "    [1, 'ABC', 200, datetime(2023, 1, 1)],\n",
    "    [2, 'ABC', 208, datetime(2023, 1, 2)],\n",
    "    [3, 'ABC', 232, datetime(2023, 1, 3)],\n",
    "    [4, 'ABC', 225, datetime(2023, 1, 4)],\n",
    "    [5, 'ABC', 226, datetime(2023, 1, 5)],\n",
    "    [6, 'XYZ', 810, datetime(2023, 1, 1)],\n",
    "    [7, 'XYZ', 803, datetime(2023, 1, 2)],\n",
    "    [8, 'XYZ', 798, datetime(2023, 1, 3)],\n",
    "    [9, 'XYZ', 795, datetime(2023, 1, 4)],\n",
    "    [10, 'XYZ', 791, datetime(2023, 1, 5)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85fd20fa",
   "metadata": {
    "hidden": true,
    "id": "85fd20fa"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import insert\n",
    "\n",
    "def insert_obs(obs):\n",
    "    stmt = insert(stocks).values(\n",
    "    obs_id=obs[0], \n",
    "    stock_ticker=obs[1], \n",
    "    price=obs[2],\n",
    "    date=obs[3]\n",
    "    )\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        conn.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6766f1f7",
   "metadata": {
    "hidden": true,
    "id": "6766f1f7"
   },
   "outputs": [],
   "source": [
    "for obs in observations:\n",
    "    insert_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9721648e",
   "metadata": {
    "hidden": true,
    "id": "9721648e"
   },
   "outputs": [],
   "source": [
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.chains import SQLDatabaseChain\n",
    "\n",
    "db = SQLDatabase(engine)\n",
    "sql_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2af8c8",
   "metadata": {
    "hidden": true,
    "id": "fe2af8c8"
   },
   "source": [
    "Finally, we will create a tool with this chain. To create a custom tool we need only three inputs:\n",
    "* name: the name of the tool (sent to the llm as context)\n",
    "* func: the function that will be applied on the LLM's request\n",
    "* description: the description of the tool, what it should be used for (sent to the llm as context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef519b1",
   "metadata": {
    "hidden": true,
    "id": "7ef519b1"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "sql_tool = Tool(\n",
    "    name='Stock DB',\n",
    "    func=sql_chain.run,\n",
    "    description=\"Useful for when you need to answer questions about stocks \" \\\n",
    "                \"and their prices.\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325f758",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "6325f758",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Agent types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea2c0c",
   "metadata": {
    "hidden": true,
    "id": "4d732b7a"
   },
   "source": [
    "In this section we will review several agents and see how they 'think' and what they can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee98631",
   "metadata": {
    "hidden": true,
    "id": "8ee98631"
   },
   "source": [
    "Using one of langchain's pre-built agents involves three variables: \n",
    "* defining the tools\n",
    "* defining the llm\n",
    "* defining the agent type\n",
    "\n",
    "This is all really easy to do in langchain, as we will see in the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf306a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "1eaf306a"
   },
   "source": [
    "### Agent type #1: Zero Shot React"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13951c49",
   "metadata": {
    "hidden": true,
    "id": "13951c49"
   },
   "source": [
    "We will be using only a few tools but you can combine the ones you prefer. Remember that tools are only utility chains under the hood which are chains that serve one specific purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcff15",
   "metadata": {
    "hidden": true,
    "id": "aabcff15"
   },
   "source": [
    "We first need to initialize the tools we'll be using in this example. Here we will want our agent to do math so we will use 'llm-math', the tool we have for solving math problems. This is one of the several tools we can load with the `load_tools` utility - you can check out more by checking out the [docs](https://langchain.readthedocs.io/en/latest/modules/agents/tools.html?highlight=tools#tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c6bd290",
   "metadata": {
    "hidden": true,
    "id": "0c6bd290"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"llm-math\"], \n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf01b924",
   "metadata": {
    "hidden": true,
    "id": "bf01b924"
   },
   "source": [
    "To be able to search across our stock prices, we will also use the custom tool we built beforehand with the sql data ('Stock DB'). We will append this tool to our list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d016d7",
   "metadata": {
    "hidden": true,
    "id": "e9d016d7"
   },
   "outputs": [],
   "source": [
    "tools.append(sql_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9bf6b5",
   "metadata": {
    "hidden": true,
    "id": "bc9bf6b5"
   },
   "source": [
    "As the name suggests, we will use this agent to perform 'zero shot' tasks on the input. That means that we will not have several, interdependent interactions but only one. In other words, this agent will have no memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e51d51",
   "metadata": {
    "hidden": true,
    "id": "a6e51d51"
   },
   "source": [
    "Now we are ready to initialize the agent! We will use `verbose` in `True` so we can see what is our agent's 'thinking' process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d95ce",
   "metadata": {
    "hidden": true,
    "id": "ca2d95ce"
   },
   "source": [
    "**Important Note:** *When interacting with agents it is really important to set the `max_iterations` parameters because agents can get stuck in infinite loops that consume plenty of tokens. The default value is 15 to allow for many tools and complex reasoning but for most applications you should keep it much lower.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4eb28c",
   "metadata": {
    "hidden": true,
    "id": "5e4eb28c"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\", \n",
    "    tools=tools, \n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c239c0",
   "metadata": {
    "hidden": true,
    "id": "28c239c0"
   },
   "source": [
    "Let's see our newly created agent in action! We will ask it a question that involves a math operation over the stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c775bc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "9c775bc6",
    "outputId": "979f31fc-c606-473d-deb3-3b1dc0222a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to compare the stock prices of 'ABC' and 'XYZ' on two different days\n",
      "Action: Stock DB\n",
      "Action Input: Stock prices of 'ABC' and 'XYZ' on January 3rd and January 4th\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Stock prices of 'ABC' and 'XYZ' on January 3rd and January 4th \n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT stock_ticker, price, date FROM stocks WHERE (stock_ticker = 'ABC' OR stock_ticker = 'XYZ') AND (date = '2023-01-03' OR date = '2023-01-04')\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('ABC', 232.0, '2023-01-03'), ('ABC', 225.0, '2023-01-04'), ('XYZ', 798.0, '2023-01-03'), ('XYZ', 795.0, '2023-01-04')]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m The stock prices of 'ABC' and 'XYZ' on January 3rd and January 4th were 232.0 and 798.0 respectively on January 3rd and 225.0 and 795.0 respectively on January 4th.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m The stock prices of 'ABC' and 'XYZ' on January 3rd and January 4th were 232.0 and 798.0 respectively on January 3rd and 225.0 and 795.0 respectively on January 4th.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the ratio between the two stock prices on each day\n",
      "Action: Calculator\n",
      "Action Input: 232.0/798.0 and 225.0/795.0\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 0.2907268170426065\n",
      "0.2830188679245283\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the multiplication of the two ratios\n",
      "Action: Calculator\n",
      "Action Input: 0.2907268170426065 * 0.2830188679245283\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 0.08228117463469994\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 2334 tokens\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    zero_shot_agent, \n",
    "    \"What is the multiplication of the ratio between stock \" +\n",
    "    \"prices for 'ABC' and 'XYZ' in January 3rd and the ratio \" + \n",
    "    \"between the same stock prices in January the 4th?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b23c42",
   "metadata": {
    "hidden": true,
    "id": "b8b23c42"
   },
   "source": [
    "As always, let's see what the prompt is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e50e81b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "3e50e81b",
    "outputId": "c1fb3eb0-01b5-46e1-f17b-d1df2bff2df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "Calculator: Useful for when you need to answer questions about math.\n",
      "Stock DB: Useful for when you need to answer questions about stocks and their prices.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Calculator, Stock DB]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(zero_shot_agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b47ed",
   "metadata": {
    "hidden": true,
    "id": "5b2b47ed"
   },
   "source": [
    "The question we must ask ourselves here is: how are agents different than chains?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73aaf07",
   "metadata": {
    "hidden": true,
    "id": "f73aaf07"
   },
   "source": [
    "If we look at the agent's logic and the prompt we have just printed we will see some clear differences. First, we have the tools which are included in the prompt. Second we have a thought process which was before was immediate in chains but now involves a 'thought', 'action', 'action input', 'observation' sequence. What is this all about?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a698f78",
   "metadata": {
    "hidden": true,
    "id": "3a698f78"
   },
   "source": [
    "Suffice it to say for now that **the LLM now has the ability to 'reason' on how to best use tools** to solve our query and can combine them in intelligent ways with just a brief description of each of them. If you want to learn more about this paradigm (MRKL) in detail, please refer to [this](https://arxiv.org/pdf/2205.00445.pdf) paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9e65d",
   "metadata": {
    "hidden": true,
    "id": "aff9e65d"
   },
   "source": [
    "Finally, let's pay attention to the 'agent_scratchpad'. What is that? Well, that is where we will be appending every thought or action that the agent has already performed. In this way, at each point in time, the agent will know what it has found out and will be able to continue its thought process. In other words, after using a tool it adds its thoughts and observations to the scratchpad and picks up from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62fb0e0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "04d70642"
   },
   "source": [
    "### Agent type #2: Conversational React"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec589b1",
   "metadata": {
    "hidden": true,
    "id": "1ec589b1"
   },
   "source": [
    "The zero shot agent is really interesting but, as we said before, it has no memory. What if we want an assistant that remembers things we have talked about and can also reason about them and use tools? For that we have the conversational react agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26c50f",
   "metadata": {
    "hidden": true,
    "id": "5c26c50f"
   },
   "source": [
    "We will use the same tools as we specified earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ae1e8e",
   "metadata": {
    "hidden": true,
    "id": "11ae1e8e"
   },
   "outputs": [],
   "source": [
    "tools = load_tools(\n",
    "    [\"llm-math\"], \n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b6faff3",
   "metadata": {
    "hidden": true,
    "id": "4b6faff3"
   },
   "outputs": [],
   "source": [
    "tools.append(sql_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45b903",
   "metadata": {
    "hidden": true,
    "id": "be45b903"
   },
   "source": [
    "The memory type being used here is a simple buffer memory to allow us to remember previous steps in the reasoning chain. For more information on memory, please refer to the 3rd chapter of this series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aff4edf",
   "metadata": {
    "hidden": true,
    "id": "0aff4edf"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6579cef0",
   "metadata": {
    "hidden": true,
    "id": "6579cef0"
   },
   "outputs": [],
   "source": [
    "conversational_agent = initialize_agent(\n",
    "    agent='conversational-react-description', \n",
    "    tools=tools, \n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cabbea50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "cabbea50",
    "outputId": "6bdd0b13-134f-4dfd-9fb7-66c746e5a07a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: Stock DB\n",
      "Action Input: ABC on January the 1st\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "ABC on January the 1st \n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-01'\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(200.0,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m The price of ABC on January the 1st was 200.0.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m The price of ABC on January the 1st was 200.0.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Is there anything else I can help you with?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 1754 tokens\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversational_agent, \n",
    "    \"Please provide me the stock prices for ABC on January the 1st\"\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd9847",
   "metadata": {
    "hidden": true,
    "id": "bdcd9847"
   },
   "source": [
    "As we can see below, the prompt is similar but it includes a great prelude of instructions that make it an effective assistant as well + a spot for including the chat history from the memory component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de413fd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "de413fd3",
    "outputId": "faf8eeb7-94df-4e92-f1a8-dfde49d1703c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "TOOLS:\n",
      "------\n",
      "\n",
      "Assistant has access to the following tools:\n",
      "\n",
      "> Calculator: Useful for when you need to answer questions about math.\n",
      "> Stock DB: Useful for when you need to answer questions about stocks and their prices.\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [Calculator, Stock DB]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "{chat_history}\n",
      "\n",
      "New input: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(conversational_agent.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee8041",
   "metadata": {
    "hidden": true,
    "id": "32ee8041"
   },
   "source": [
    "The conversational assitant is towards conversation and has a bit more trouble combining a complex combination of tools. Let's see what happens if we make it try and answer the same question we posed to our previous agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e109878",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "5e109878",
    "outputId": "6d7cab70-338e-4d88-9257-f34dee165514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: Stock DB\n",
      "Action Input: Get the ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd and the ratio of the same prices of the same stocks in January the 4th\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
      "Get the ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd and the ratio of the same prices of the same stocks in January the 4th \n",
      "SQLQuery:\u001b[32;1m\u001b[1;3m SELECT (SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-03') / (SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-03') AS ratio_jan_3, (SELECT price FROM stocks WHERE stock_ticker = 'ABC' AND date = '2023-01-04') / (SELECT price FROM stocks WHERE stock_ticker = 'XYZ' AND date = '2023-01-04') AS ratio_jan_4 FROM stocks LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(0.2907268170426065, 0.2830188679245283), (0.2907268170426065, 0.2830188679245283), (0.2907268170426065, 0.2830188679245283), (0.2907268170426065, 0.2830188679245283), (0.2907268170426065, 0.2830188679245283)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3m The ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd is 0.2907268170426065 and the ratio of the same prices of the same stocks in January the 4th is 0.2830188679245283.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3m The ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd is 0.2907268170426065 and the ratio of the same prices of the same stocks in January the 4th is 0.2830188679245283.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The answer is 0.4444444444444444. Is there anything else I can help you with?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 2518 tokens\n"
     ]
    }
   ],
   "source": [
    "result = count_tokens(\n",
    "    conversational_agent, \n",
    "    \"What is the multiplication of the ratio of the prices of stocks 'ABC' and 'XYZ' in January 3rd and the ratio of the same prices of the same stocks in January the 4th?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11c646",
   "metadata": {
    "hidden": true,
    "id": "fe11c646"
   },
   "source": [
    "As we can see, it solves the problem but in a different manner, by using the SQL tool to solve the math as well. Interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44135b8d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "44135b8d"
   },
   "source": [
    "### Agent type #3: React Docstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d0d13",
   "metadata": {
    "hidden": true,
    "id": "1e6d0d13"
   },
   "source": [
    "This type of agent is similar to the ones we have seen so far but it includes the interaction with a docstore. It will have two and only two tools at its disposal: 'Search' and 'Lookup'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569fd41",
   "metadata": {
    "hidden": true,
    "id": "d569fd41"
   },
   "source": [
    "With 'Search' it will bring up a relevant article and with 'Lookup' the agent will find the right piece of information in the article. This is probably easiest to see in an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecc452af",
   "metadata": {
    "hidden": true,
    "id": "ecc452af"
   },
   "outputs": [],
   "source": [
    "from langchain import Wikipedia\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "\n",
    "docstore=DocstoreExplorer(Wikipedia())\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=docstore.search,\n",
    "        description='search wikipedia'\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Lookup\",\n",
    "        func=docstore.lookup,\n",
    "        description='lookup a term in wikipedia'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "595938a1",
   "metadata": {
    "hidden": true,
    "id": "595938a1"
   },
   "outputs": [],
   "source": [
    "docstore_agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=\"react-docstore\", \n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bba6b065",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "bba6b065",
    "outputId": "40179869-4d21-432c-fba3-2f3f548b3034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search Archimedes and find his last words.\n",
      "Action: Search[Archimedes]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mArchimedes of Syracuse (; c. 287 – c. 212 BC) was a Greek mathematician, physicist, engineer, astronomer, and inventor from the ancient city of Syracuse in Sicily. Although few details of his life are known, he is regarded as one of the leading scientists in classical antiquity. Considered the greatest mathematician of ancient history, and one of the greatest of all time, Archimedes anticipated modern calculus and analysis by applying the concept of the infinitely small and the method of exhaustion to derive and rigorously prove a range of geometrical theorems. These include the area of a circle, the surface area and volume of a sphere, the area of an ellipse, the area under a parabola, the volume of a segment of a paraboloid of revolution, the volume of a segment of a hyperboloid of revolution, and the area of a spiral.Archimedes' other mathematical achievements include deriving an approximation of pi, defining and investigating the Archimedean spiral, and devising a system using exponentiation for expressing very large numbers. He was also one of the first to apply mathematics to physical phenomena, working on statics and hydrostatics. Archimedes' achievements in this area include a proof of the law of the lever, the widespread use of the concept of center of gravity, and the enunciation of the law of buoyancy or Archimedes' principle. He is also credited with designing innovative machines, such as his screw pump, compound pulleys, and defensive war machines to protect his native Syracuse from invasion.\n",
      "Archimedes died during the siege of Syracuse, when he was killed by a Roman soldier despite orders that he should not be harmed. Cicero describes visiting Archimedes' tomb, which was surmounted by a sphere and a cylinder that Archimedes requested be placed there to represent his mathematical discoveries.\n",
      "Unlike his inventions, Archimedes' mathematical writings were little known in antiquity. Mathematicians from Alexandria read and quoted him, but the first comprehensive compilation was not made until c. 530 AD by Isidore of Miletus in Byzantine Constantinople, while commentaries on the works of Archimedes by Eutocius in the 6th century opened them to wider readership for the first time. The relatively few copies of Archimedes' written work that survived through the Middle Ages were an influential source of ideas for scientists during the Renaissance and again in the 17th century, while the discovery in 1906 of previously lost works by Archimedes in the Archimedes Palimpsest has provided new insights into how he obtained mathematical results.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The paragraph does not mention Archimedes' last words. I need to look up \"last words\".\n",
      "Action: Lookup[last words]\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m(Result 1/1) Plutarch (45–119 AD) wrote in his Parallel Lives that Archimedes was related to King Hiero II, the ruler of Syracuse. He also provides at least two accounts on how Archimedes died after the city was taken. According to the most popular account, Archimedes was contemplating a mathematical diagram when the city was captured. A Roman soldier commanded him to come and meet Marcellus, but he declined, saying that he had to finish working on the problem. This enraged the soldier, who killed Archimedes with his sword. Another story has Archimedes carrying mathematical instruments before being killed because a soldier thought they were valuable items. Marcellus was reportedly angered by Archimedes' death, as he considered him a valuable scientific asset (he called Archimedes \"a geometrical Briareus\") and had ordered that he should not be harmed.The last words attributed to Archimedes are \"Do not disturb my circles\" (Latin, \"Noli turbare circulos meos\"; Katharevousa Greek, \"μὴ μου τοὺς κύκλους τάραττε\"), a reference to the mathematical drawing that he was supposedly studying when disturbed by the Roman soldier. There is no reliable evidence that Archimedes uttered these words and they do not appear in Plutarch's account. A similar quotation is found in the work of Valerius Maximus (fl. 30 AD), who wrote in Memorable Doings and Sayings, \"... sed protecto manibus puluere 'noli' inquit, 'obsecro, istum disturbare'\" (\"... but protecting the dust with his hands, said 'I beg of you, do not disturb this'\").\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The last words attributed to Archimedes are \"Do not disturb my circles\", so the answer is \"Do not disturb my circles\".\n",
      "Action: Finish[\"Do not disturb my circles\"]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 6064 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What were Archimedes' last words?\",\n",
       " 'output': '\"Do not disturb my circles\"'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(docstore_agent, \"What were Archimedes' last words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f77ac",
   "metadata": {
    "hidden": true,
    "id": "6a5f77ac"
   },
   "source": [
    "We will not print the prompt here because it is too large, but you can see it yourself if you want to (we know how to already).\n",
    "\n",
    "In short, it contains several examples of the `Question` > `Thought` > `Action` > `Observation` loop, that include the `Search` and `Lookup` tools.\n",
    "\n",
    "If you want to learn more about this approach [this](https://arxiv.org/pdf/2210.03629.pdf) is the paper for ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae3b5b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "4aae3b5b"
   },
   "source": [
    "### Agent type #4: Self Ask with Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07039e5",
   "metadata": {
    "hidden": true,
    "id": "c07039e5"
   },
   "source": [
    "This is the first-choice agent to use when using LLM's to extract information with a search engine. The agent will ask follow-up questions and use the search functionality to get intermediate answers that help it get to a final answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903660b2",
   "metadata": {
    "hidden": true,
    "id": "903660b2"
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key='api_key')\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description='google search'\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=\"self-ask-with-search\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1c4c8",
   "metadata": {
    "hidden": true,
    "id": "0ec1c4c8"
   },
   "source": [
    "We will not interact with this agent because for that we would need a serpapi key. However, by checking out the prompt we can see a few examples of how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4134930",
   "metadata": {
    "hidden": true,
    "id": "e4134930",
    "outputId": "eed4d667-5e20-4db8-e8e2-75313c022928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(self_ask_with_search.agent.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7790ae",
   "metadata": {
    "hidden": true,
    "id": "3e7790ae"
   },
   "source": [
    "As we can see, the prompt is basically a series of many examples to show the LLM how to ask follow up questions to a search tool until it can get to the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df324ba",
   "metadata": {
    "hidden": true,
    "id": "5df324ba"
   },
   "source": [
    "And.. again [here](https://arxiv.org/pdf/2210.03350.pdf) you have the paper to dive deeper!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd80a08",
   "metadata": {
    "hidden": true,
    "id": "4cd80a08"
   },
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d108e2",
   "metadata": {
    "hidden": true,
    "id": "97d108e2"
   },
   "source": [
    "And that all for agents! There's many other things you can do with agents, just to name a few:\n",
    "* Create your own custom agent\n",
    "* Use them with many other tools (even custom ones)\n",
    "* Trace every call an agent makes through a convinient UI interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33dd90a",
   "metadata": {
    "hidden": true,
    "id": "e33dd90a"
   },
   "source": [
    "Check the how-to [guides](https://langchain.readthedocs.io/en/latest/modules/agents/how_to_guides.html) for more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b3813",
   "metadata": {
    "heading_collapsed": true,
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Retrieval Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d10357",
   "metadata": {
    "hidden": true,
    "id": "bhWwrfbbVGOA"
   },
   "source": [
    "We've seen in previous chapters how powerful [retrieval augmentation](https://www.pinecone.io/learn/langchain-retrieval-augmentation/) and [conversational agents](https://www.pinecone.io/learn/langchain-agents/) can be. They become even more impressive when we begin using them together.\n",
    "\n",
    "Conversational agents can struggle with data freshness, knowledge about specific domains, or accessing internal documentation. By coupling agents with retrieval augmentation tools we no longer have these problems.\n",
    "\n",
    "One the other side, using \"naive\" retrieval augmentation without the use of an agent means we will retrieve contexts with *every* query. Again, this isn't always ideal as not every query requires access to external knowledge.\n",
    "\n",
    "Merging these methods gives us the best of both worlds. In this notebook we'll learn how to do this.\n",
    "\n",
    "To begin, we must install the prerequisite libraries that we will be using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e6e7f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "pva9ehKXUpU2",
    "outputId": "3bbcf2dd-1889-412f-d45a-f56945ac4f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    openai==0.27.7 \\\n",
    "    \"pinecone-client[grpc]\"==2.2.1 \\\n",
    "    langchain==0.0.162 \\\n",
    "    tiktoken==0.4.0 \\\n",
    "    datasets==2.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb79c35",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "ZTgrOQziXUto"
   },
   "source": [
    "## Building the Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8caf9c",
   "metadata": {
    "hidden": true,
    "id": "qNyRsz0ZXXaq"
   },
   "source": [
    "We start by constructing our knowledge base. We'll use a mostly prepared dataset called **S**tanford **Qu**estion-**A**nswering **D**ataset (SQuAD) hosted on Hugging Face *Datasets*. We download it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371be4af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "laSDMjqQXuj-",
    "outputId": "5272df99-eb4b-4ec2-c513-504e067be2b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('squad', split='train')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbb11c",
   "metadata": {
    "hidden": true,
    "id": "8casaLEpX18U"
   },
   "source": [
    "The dataset does contain duplicate contexts, which we can remove like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd84563b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "hidden": true,
    "id": "JnWZTcJiXzor",
    "outputId": "42659cec-23c3-4349-b007-676da99d834d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b36c3610-a2f2-4edd-8630-b26ad29bc118\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b36c3610-a2f2-4edd-8630-b26ad29bc118')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b36c3610-a2f2-4edd-8630-b26ad29bc118 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b36c3610-a2f2-4edd-8630-b26ad29bc118');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2cbb9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "hidden": true,
    "id": "er2nM4vsYD4R",
    "outputId": "d58b5640-be25-495a-b90c-585940c851bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7c7fcc31-6db4-414e-be73-6ad623834112\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733bf84d058e614000b61be</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>As at most other universities, Notre Dame's st...</td>\n",
       "      <td>When did the Scholastic Magazine of Notre dame...</td>\n",
       "      <td>{'text': ['September 1876'], 'answer_start': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5733bed24776f41900661188</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The university is the major seat of the Congre...</td>\n",
       "      <td>Where is the headquarters of the Congregation ...</td>\n",
       "      <td>{'text': ['Rome'], 'answer_start': [119]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5733a6424776f41900660f51</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The College of Engineering was established in ...</td>\n",
       "      <td>How many BS level degrees are offered in the C...</td>\n",
       "      <td>{'text': ['eight'], 'answer_start': [487]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>All of Notre Dame's undergraduate students are...</td>\n",
       "      <td>What entity provides help with the management ...</td>\n",
       "      <td>{'text': ['Learning Resource Center'], 'answer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c7fcc31-6db4-414e-be73-6ad623834112')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7c7fcc31-6db4-414e-be73-6ad623834112 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7c7fcc31-6db4-414e-be73-6ad623834112');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                          id                     title  \\\n",
       "0   5733be284776f41900661182  University_of_Notre_Dame   \n",
       "5   5733bf84d058e614000b61be  University_of_Notre_Dame   \n",
       "10  5733bed24776f41900661188  University_of_Notre_Dame   \n",
       "15  5733a6424776f41900660f51  University_of_Notre_Dame   \n",
       "20  5733a70c4776f41900660f64  University_of_Notre_Dame   \n",
       "\n",
       "                                              context  \\\n",
       "0   Architecturally, the school has a Catholic cha...   \n",
       "5   As at most other universities, Notre Dame's st...   \n",
       "10  The university is the major seat of the Congre...   \n",
       "15  The College of Engineering was established in ...   \n",
       "20  All of Notre Dame's undergraduate students are...   \n",
       "\n",
       "                                             question  \\\n",
       "0   To whom did the Virgin Mary allegedly appear i...   \n",
       "5   When did the Scholastic Magazine of Notre dame...   \n",
       "10  Where is the headquarters of the Congregation ...   \n",
       "15  How many BS level degrees are offered in the C...   \n",
       "20  What entity provides help with the management ...   \n",
       "\n",
       "                                              answers  \n",
       "0   {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "5   {'text': ['September 1876'], 'answer_start': [...  \n",
       "10          {'text': ['Rome'], 'answer_start': [119]}  \n",
       "15         {'text': ['eight'], 'answer_start': [487]}  \n",
       "20  {'text': ['Learning Resource Center'], 'answer...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(subset='context', keep='first', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945677af",
   "metadata": {
    "hidden": true,
    "id": "B2_Pt7N6Zg2X"
   },
   "source": [
    "### Initialize the Embedding Model and Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3014be",
   "metadata": {
    "hidden": true,
    "id": "bGoS84KYZnSK"
   },
   "source": [
    "We'll be using OpenAI's `text-embedding-ada-002` model initialize via LangChain and the Pinecone vector DB. We start by initializing the embedding model, for this we need an [OpenAI API key](https://platform.openai.com/).\n",
    "\n",
    "*(Note that OpenAI is a paid service and so running the remainder of this notebook may incur some small cost)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbbad04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "U57x2_87YSpb",
    "outputId": "cf4c99af-24b7-4bf5-97ea-71091c8fc2ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ··········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")  # platform.openai.com\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c461e08",
   "metadata": {
    "hidden": true,
    "id": "JQTfOTR6aBRS"
   },
   "source": [
    "Next we initialize the vector database. For this we need a [free API key](https://app.pinecone.io/), then we create the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9631c005",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "C3wrG-9yaJel",
    "outputId": "842bf46f-fd0f-4322-8d90-a7fd769b7687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone API Key: ··········\n",
      "Pinecone environment: us-west1-gcp\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "# find API key in console at app.pinecone.io\n",
    "YOUR_API_KEY = getpass(\"Pinecone API Key: \")\n",
    "# find ENV (cloud region) next to API key in console\n",
    "YOUR_ENV = input(\"Pinecone environment: \")\n",
    "\n",
    "index_name = 'langchain-retrieval-agent'\n",
    "pinecone.init(\n",
    "    api_key=YOUR_API_KEY,\n",
    "    environment=YOUR_ENV\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='dotproduct',\n",
    "        dimension=1536  # 1536 dim of text-embedding-ada-002\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28f0a3",
   "metadata": {
    "hidden": true,
    "id": "uiSWrAQ5aRco"
   },
   "source": [
    "Then connect to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b657310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "bfsfuFmqaS4G",
    "outputId": "3623e209-7bd5-4cea-8b18-92265b6bee4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7c972",
   "metadata": {
    "hidden": true,
    "id": "AD5IGOoLaVx7"
   },
   "source": [
    "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
    "\n",
    "## Indexing\n",
    "\n",
    "We can perform the indexing task using the LangChain vector store object. But for now it is much faster to do it via the Pinecone python client directly. We will do this in batches of `100` or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34a60da8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "321ebcd192fc41e0a7a2ba9c0e7b8556",
      "7c898f43b39545879013df760aa4f6f1",
      "1b5fe655986842a6afd4a2eb2c0f47fc",
      "76872ca737b64616aeacc90f2f7fc655",
      "0679733cce4f4b5eb25f8f56dc7ebe22",
      "71551fc297b54fb4a21d787d117e01a1",
      "b90283522431447b925b226431bb4fa6",
      "a03d372e8b4f456e8663e54c818e0fe9",
      "6fc773ccbc594fddbc1f2f0adaa96186",
      "07b47869f727455fb5ba67e99767c04c",
      "4b2e12a33584489690aaf69f51cd4a3b"
     ]
    },
    "hidden": true,
    "id": "AhDcbRGTaWPi",
    "outputId": "dca5a4d0-5d19-4542-c26c-3073bfa13f2a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321ebcd192fc41e0a7a2ba9c0e7b8556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # get end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # first get metadata fields for this record\n",
    "    metadatas = [{\n",
    "        'title': record['title'],\n",
    "        'text': record['context']\n",
    "    } for j, record in batch.iterrows()]\n",
    "    # get the list of contexts / documents\n",
    "    documents = batch['context']\n",
    "    # create document embeddings\n",
    "    embeds = embed.embed_documents(documents)\n",
    "    # get IDs\n",
    "    ids = batch['id']\n",
    "    # add everything to pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7bf69e",
   "metadata": {
    "hidden": true,
    "id": "jDUnLdy1b7G1"
   },
   "source": [
    "We've indexed everything, now we can check the number of vectors in our index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1921a6fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "SiccGZKAb_Qo",
    "outputId": "5cafd8c8-771f-46d0-e261-12c9b5b1b058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 18891}},\n",
       " 'total_vector_count': 18891}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d16e7a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "b-3oolT5cCR8"
   },
   "source": [
    "## Creating a Vector Store and Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383389b",
   "metadata": {
    "hidden": true,
    "id": "DcZ12U06cCH5"
   },
   "source": [
    "Now that we've build our index we can switch back over to LangChain. We start by initializing a vector store using the same index we just built. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2cf49a9",
   "metadata": {
    "hidden": true,
    "id": "0MBJ477-cFNw"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f49c66",
   "metadata": {
    "hidden": true,
    "id": "3K3xRthWcXzW"
   },
   "source": [
    "As in previous examples, we can use the `similarity_search` method to do a pure semantic search (without the generation component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9a4ebc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "uITMZtzschJF",
    "outputId": "88fae02f-6f82-47d7-a153-21b3e0615709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow over the years, adding more colleges, programs, and sports teams. By 1921, with the addition of the College of Commerce, Notre Dame had grown from a small college to a university with five colleges and a professional law school. The university continued to expand and add new residence halls and buildings with each subsequent president.\", metadata={'title': 'University_of_Notre_Dame'}),\n",
       " Document(page_content='The College of Engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the College of Science since the 1870s. Today the college, housed in the Fitzpatrick, Cushing, and Stinson-Remick Halls of Engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight B.S. degrees offered. Additionally, the college offers five-year dual degree programs with the Colleges of Arts and Letters and of Business awarding additional B.A. and Master of Business Administration (MBA) degrees, respectively.', metadata={'title': 'University_of_Notre_Dame'}),\n",
       " Document(page_content='Since 2005, Notre Dame has been led by John I. Jenkins, C.S.C., the 17th president of the university. Jenkins took over the position from Malloy on July 1, 2005. In his inaugural address, Jenkins described his goals of making the university a leader in research that recognizes ethics and building the connection between faith and studies. During his tenure, Notre Dame has increased its endowment, enlarged its student body, and undergone many construction projects on campus, including Compton Family Ice Arena, a new architecture hall, additional residence halls, and the Campus Crossroads, a $400m enhancement and expansion of Notre Dame Stadium.', metadata={'title': 'University_of_Notre_Dame'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"when was the college of engineering in the University of Notre Dame established?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d431e",
   "metadata": {
    "hidden": true,
    "id": "-zGF6YsgczqT"
   },
   "source": [
    "Looks like we're getting good results. Let's take a look at how we can begin integrating this into a conversational agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fd18c",
   "metadata": {
    "hidden": true,
    "id": "tFsIOm73dcOI"
   },
   "source": [
    "## Initializing the Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4551e",
   "metadata": {
    "hidden": true,
    "id": "XMv6TXWkdfNR"
   },
   "source": [
    "Our conversational agent needs a Chat LLM, conversational memory, and a `RetrievalQA` chain to initialize. We create these using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f7ebc05",
   "metadata": {
    "hidden": true,
    "id": "zMRs9Klic5-Y"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696ba50",
   "metadata": {
    "hidden": true,
    "id": "-ySfWyZLdboX"
   },
   "source": [
    "Using these we can generate an answer using the `run` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8efbec1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "hidden": true,
    "id": "LaYSq0V-dxHw",
    "outputId": "a85730a3-ab15-49b6-8aba-6da774fd3e3f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The College of Engineering was established in 1920 at the University of Notre Dame.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74e6c0",
   "metadata": {
    "hidden": true,
    "id": "DtSXR5RXdyU0"
   },
   "source": [
    "But this isn't yet ready for our conversational agent. For that we need to convert this retrieval chain into a tool. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "577fd913",
   "metadata": {
    "hidden": true,
    "id": "FwCYrS4duqBW"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a1cde",
   "metadata": {
    "hidden": true,
    "id": "wXi_0ipTvM_l"
   },
   "source": [
    "Now we can initialize the agent like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0985ed7c",
   "metadata": {
    "hidden": true,
    "id": "JaKTzPUEvOoy"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea84463",
   "metadata": {
    "hidden": true,
    "id": "WbXl-AzVvszB"
   },
   "source": [
    "With that our retrieval augmented conversational agent is ready and we can begin using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3315b",
   "metadata": {
    "hidden": true,
    "id": "IlxUBWKcvzeP"
   },
   "source": [
    "### Using the Conversational Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e56a09",
   "metadata": {
    "hidden": true,
    "id": "ZZapCP4Pv2kz"
   },
   "source": [
    "To make queries we simply call the `agent` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "643fe988",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "RJoAhy76vzAB",
    "outputId": "31ae7589-e66a-4835-d1d8-555bb59c962c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"When was the College of Engineering at the University of Notre Dame established?\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe College of Engineering at the University of Notre Dame was established in 1920.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The College of Engineering at the University of Notre Dame was established in 1920.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'when was the college of engineering in the University of Notre Dame established?',\n",
       " 'chat_history': [],\n",
       " 'output': 'The College of Engineering at the University of Notre Dame was established in 1920.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9cadf",
   "metadata": {
    "hidden": true,
    "id": "YcMqa9Va2hU6"
   },
   "source": [
    "Looks great, now what if we ask it a non-general knowledge question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2daac75d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "85vipqC02deV",
    "outputId": "345c724e-eaea-4a20-9445-99794bc743fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The result of 2 * 7 is 14.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 2 * 7?',\n",
       " 'chat_history': [HumanMessage(content='when was the college of engineering in the University of Notre Dame established?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The College of Engineering at the University of Notre Dame was established in 1920.', additional_kwargs={}, example=False)],\n",
       " 'output': 'The result of 2 * 7 is 14.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"what is 2 * 7?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfc383",
   "metadata": {
    "hidden": true,
    "id": "gR_b0IN32rQ9"
   },
   "source": [
    "Perfect, the agent is able to recognize that it doesn't need to refer to it's general knowledge tool for that question. Let's try some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eca003d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "mQeicHTj2pmY",
    "outputId": "9810647d-0846-4ca8-ae2e-1b249ca4ad3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"University of Notre Dame facts\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m- The University of Notre Dame is a Catholic research university located in South Bend, Indiana, USA.\n",
      "- It is consistently ranked among the top twenty universities in the United States and as a major global university.\n",
      "- The undergraduate component of the university is organized into four colleges (Arts and Letters, Science, Engineering, Business) and the Architecture School.\n",
      "- The university's graduate program has more than 50 master's, doctoral and professional degree programs offered by the five schools, with the addition of the Notre Dame Law School and a MD-PhD program offered in combination with IU medical School.\n",
      "- Notre Dame's campus covers 1,250 acres in a suburban setting and it contains a number of recognizable landmarks, such as the Golden Dome, the \"Word of Life\" mural (commonly known as Touchdown Jesus), and the Basilica.\n",
      "- The university counts approximately 120,000 alumni, considered among the strongest alumni networks among U.S. colleges.\n",
      "- The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns.\n",
      "- Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet.\n",
      "- In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students.\n",
      "- The university's intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where \"Everyone Plays Intramural Sports.\"\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The University of Notre Dame is a Catholic research university located in South Bend, Indiana, USA. It is consistently ranked among the top twenty universities in the United States and as a major global university. The undergraduate component of the university is organized into four colleges (Arts and Letters, Science, Engineering, Business) and the Architecture School. The university's graduate program has more than 50 master's, doctoral and professional degree programs offered by the five schools, with the addition of the Notre Dame Law School and a MD-PhD program offered in combination with IU medical School. Notre Dame's campus covers 1,250 acres in a suburban setting and it contains a number of recognizable landmarks, such as the Golden Dome, the \\\"Word of Life\\\" mural (commonly known as Touchdown Jesus), and the Basilica. The university counts approximately 120,000 alumni, considered among the strongest alumni networks among U.S. colleges. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. The university's intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where \\\"Everyone Plays Intramural Sports.\\\"\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you tell me some facts about the University of Notre Dame?',\n",
       " 'chat_history': [HumanMessage(content='when was the college of engineering in the University of Notre Dame established?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The College of Engineering at the University of Notre Dame was established in 1920.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is 2 * 7?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The result of 2 * 7 is 14.', additional_kwargs={}, example=False)],\n",
       " 'output': 'The University of Notre Dame is a Catholic research university located in South Bend, Indiana, USA. It is consistently ranked among the top twenty universities in the United States and as a major global university. The undergraduate component of the university is organized into four colleges (Arts and Letters, Science, Engineering, Business) and the Architecture School. The university\\'s graduate program has more than 50 master\\'s, doctoral and professional degree programs offered by the five schools, with the addition of the Notre Dame Law School and a MD-PhD program offered in combination with IU medical School. Notre Dame\\'s campus covers 1,250 acres in a suburban setting and it contains a number of recognizable landmarks, such as the Golden Dome, the \"Word of Life\" mural (commonly known as Touchdown Jesus), and the Basilica. The university counts approximately 120,000 alumni, considered among the strongest alumni networks among U.S. colleges. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. The university\\'s intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where \"Everyone Plays Intramural Sports.\"'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"can you tell me some facts about the University of Notre Dame?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09273d33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "G93vLXso3B5Z",
    "outputId": "cb345147-5630-4a5a-bb9f-242ad91d9968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The University of Notre Dame is a Catholic research university located in South Bend, Indiana, USA. It is consistently ranked among the top twenty universities in the United States and as a major global university.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you summarize these facts in two short sentences',\n",
       " 'chat_history': [HumanMessage(content='when was the college of engineering in the University of Notre Dame established?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The College of Engineering at the University of Notre Dame was established in 1920.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='what is 2 * 7?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The result of 2 * 7 is 14.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='can you tell me some facts about the University of Notre Dame?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The University of Notre Dame is a Catholic research university located in South Bend, Indiana, USA. It is consistently ranked among the top twenty universities in the United States and as a major global university. The undergraduate component of the university is organized into four colleges (Arts and Letters, Science, Engineering, Business) and the Architecture School. The university\\'s graduate program has more than 50 master\\'s, doctoral and professional degree programs offered by the five schools, with the addition of the Notre Dame Law School and a MD-PhD program offered in combination with IU medical School. Notre Dame\\'s campus covers 1,250 acres in a suburban setting and it contains a number of recognizable landmarks, such as the Golden Dome, the \"Word of Life\" mural (commonly known as Touchdown Jesus), and the Basilica. The university counts approximately 120,000 alumni, considered among the strongest alumni networks among U.S. colleges. The university has many multi-disciplinary institutes devoted to research in varying fields, including the Medieval Institute, the Kellogg Institute for International Studies, the Kroc Institute for International Peace studies, and the Center for Social Concerns. Recent research includes work on family conflict and child development, genome mapping, the increasing trade deficit of the United States with China, studies in fluid mechanics, computational science and engineering, and marketing trends on the Internet. In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. The university\\'s intramural sports program was named in 2004 by Sports Illustrated as the best program in the country, while in 2007 The Princeton Review named it as the top school where \"Everyone Plays Intramural Sports.\"', additional_kwargs={}, example=False)],\n",
       " 'output': 'The University of Notre Dame is a Catholic research university located in South Bend, Indiana, USA. It is consistently ranked among the top twenty universities in the United States and as a major global university.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"can you summarize these facts in two short sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbc68f",
   "metadata": {
    "hidden": true,
    "id": "PWivmw9F3bCw"
   },
   "source": [
    "Looks great! We're also able to ask questions that refer to previous interactions in the conversation and the agent is able to refer to the conversation history to as a source of information.\n",
    "\n",
    "That's all for this example of building a retrieval augmented conversational agent with OpenAI and Pinecone (the OP stack) and LangChain.\n",
    "\n",
    "Once finished, we delete the Pinecone index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5651d137",
   "metadata": {
    "hidden": true,
    "id": "Pa1whr8V3Wfm"
   },
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb2605",
   "metadata": {
    "hidden": true,
    "id": "Ykg5TYA033yR"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959470c3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeede90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
