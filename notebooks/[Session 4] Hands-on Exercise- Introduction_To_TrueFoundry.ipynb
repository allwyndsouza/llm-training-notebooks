{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMdypEsuY1gh"
      },
      "source": [
        "# What is TrueFoundry‚ùì‚ùì‚ùì\n",
        "1. An ML Platforms - training jobs, model registry, experiment tracking, inference services, notebooks\n",
        "2. Deployments - deploy code in minutes, multi-cloud, best practices out of the box\n",
        "3. Software Catalogues - manage users, provisions resources, all your workloads under one place\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5CP2L4KZ4rq"
      },
      "source": [
        "#ü§î How does it work?\n",
        "![image](https://i.imgur.com/MTr5wcZ.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB16SiWrbNrE"
      },
      "source": [
        "#üíª What can you do with TrueFoundry?\n",
        "1. Deploy a service from code on Github\n",
        "2. Setup a cron job using Python code\n",
        "3. Create an ML repo and store your artifacts\n",
        "4. More Below üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z85dHUr5J8Yc"
      },
      "outputs": [],
      "source": [
        "!pip install -q mlfoundry servicefoundry transformers datasets transformers[torch] gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT4q8EJlc_EI"
      },
      "source": [
        "# Let's get setup üèã"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJz1Sk1hKDRu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "import logging\n",
        "import random\n",
        "import string\n",
        "\n",
        "print(random.choices(string.ascii_lowercase))\n",
        "[logging.root.removeHandler(h) for h in logging.root.handlers]\n",
        "logging.basicConfig(level=logging.WARN, format='%(asctime)s [%(name)s] %(levelname)-8s %(message)s')\n",
        "\n",
        "os.environ[\"TFY_HOST\"] = \"https://carelon.truefoundry.cloud\"\n",
        "print(f\"Set API Key (You can generate one at {os.getenv('TFY_HOST')}/settings/?tab=api-keys)\")\n",
        "os.environ[\"TFY_API_KEY\"] = getpass.getpass(f\"Enter API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yDDl8zM8o8G"
      },
      "outputs": [],
      "source": [
        "REPO_NAME = input('give a unique name for your ML repository: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHGkjcE4dGzO"
      },
      "source": [
        "# Creating a ML repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXGVr6a_STwW"
      },
      "outputs": [],
      "source": [
        "import mlfoundry\n",
        "mlf_client = mlfoundry.get_client()\n",
        "repo = mlf_client.create_ml_repo(REPO_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp8MMWIGdJ7v"
      },
      "source": [
        "# What is experiment tracking?\n",
        "Quiz?\n",
        "1. What are the different things we track during an experiment?\n",
        "2. Why is it important?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX96fyXVV5QW"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset as an example\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# create an MLFoundry run\n",
        "run = mlf_client.create_run(ml_repo=REPO_NAME, run_name='sklearn-run')\n",
        "run.set_tags({\n",
        "    'dataset': 'iris'\n",
        "})\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
        "\n",
        "# store our hyperparams\n",
        "run.log_params({\n",
        "    'test_size': 0.7,\n",
        "    'random_state': 42\n",
        "})\n",
        "\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Log metrics\n",
        "# store our hyperparams\n",
        "run.log_metrics({\n",
        "    'accuracy': accuracy\n",
        "})\n",
        "\n",
        "# Log model\n",
        "model_version = run.log_model(\n",
        "    name=\"my-sklearn-model\",\n",
        "    model=model,\n",
        "    framework=\"sklearn\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y85jSQ0dbBt"
      },
      "source": [
        "# Tracking a TrueFoundry job\n",
        "https://github.com/truefoundry/llm-training-notebooks/tree/main/jobs/iris-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe8AtnRfdXzm"
      },
      "source": [
        "# Now let's track an LLM training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsGifvEfSYpG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2NqGik6BEeG"
      },
      "outputs": [],
      "source": [
        "# let's id the model and load datasets\n",
        "TASK = \"classification\"\n",
        "DATASET = \"tweet_eval\"\n",
        "SUBSET = \"emotion\"\n",
        "MODEL_CHECKPOINT = \"google/bert_uncased_L-2_H-128_A-2\"\n",
        "\n",
        "dataset = load_dataset(DATASET, SUBSET)\n",
        "metric = load_metric(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrrBfrmBBG7P"
      },
      "outputs": [],
      "source": [
        "# sample data\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgvA7NaTBJXJ"
      },
      "outputs": [],
      "source": [
        "fake_preds = np.random.randint(0, 2, size=(64,))\n",
        "fake_labels = np.random.randint(0, 2, size=(64,))\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5LQ6q8PBN0U"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
        "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LWHrtMlBSAc"
      },
      "outputs": [],
      "source": [
        "sentence1_key, sentence2_key = \"text\", None\n",
        "if sentence2_key is None:\n",
        "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
        "else:\n",
        "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
        "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEwwei_xBVtm"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    if sentence2_key is None:\n",
        "        return tokenizer(examples[sentence1_key], truncation=True, max_length=256)\n",
        "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moc2w2-ZBXmt"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMSlVuGBBZi7"
      },
      "outputs": [],
      "source": [
        "num_labels = dataset['train'].features['label'].num_classes\n",
        "labels = dataset['train'].features['label'].names\n",
        "label2id = dict(zip(labels, range(len(labels))))\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PVci3_tBbKU"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKtPVLR0Bcxw"
      },
      "outputs": [],
      "source": [
        "# load the model and config for training\n",
        "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT, label2id=label2id, id2label=id2label)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9J2OPK1Bf3k"
      },
      "outputs": [],
      "source": [
        "metric_name = \"accuracy\"\n",
        "batch_size = 256\n",
        "epochs = 20\n",
        "freq = 20\n",
        "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
        "\n",
        "# trainer arguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-{DATASET}-{SUBSET}-{TASK}\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=freq,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=freq,\n",
        "    save_total_limit=3,\n",
        "    learning_rate=9e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    warmup_ratio=0.5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    push_to_hub=False,\n",
        "    report_to=[],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hCDnV4zCJj0"
      },
      "outputs": [],
      "source": [
        "from mlfoundry.integrations.transformers import MlFoundryTrainerCallback, LogModelStrategy\n",
        "\n",
        "# this callback is used to log our metrics automatically\n",
        "mlf_cb = MlFoundryTrainerCallback(\n",
        "    ml_repo=REPO_NAME,\n",
        "    run_name=f\"{model_name}-finetuned-hf\".replace(\"_\", \"-\"),\n",
        "    flatten_params=True,\n",
        "    log_model_strategy=LogModelStrategy.BEST_ONLY\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqtmyIekCUrI"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[mlf_cb]\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "uouJxqCAfUkj",
        "outputId": "c348a972-645d-45d7-9ad2-3d7d5d77ec44"
      },
      "outputs": [],
      "source": [
        "run_fqn = input('let us fetch the run fqn from the mlrepo: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHjr5x5oK3wY"
      },
      "outputs": [],
      "source": [
        "from mlfoundry.integrations.transformers import HF_MODEL_PATH\n",
        "\n",
        "run = mlf_client.get_run_by_fqn(run_fqn)\n",
        "downloaded = run.download_artifact_deprecated(HF_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWRUOTRgL_KG"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(downloaded)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(downloaded, config=config)\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggQcBQVQMDPu"
      },
      "outputs": [],
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
        "# outputs a list of dicts like [[{'label': 'NEGATIVE', 'score': 0.0001223755971295759},  {'label': 'POSITIVE', 'score': 0.9998776316642761}]]\n",
        "pipe(\"I am not feeling great\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UycG-pRzY-9n"
      },
      "outputs": [],
      "source": [
        "def predict(input):\n",
        "  output = pipe(input)\n",
        "  return {obj['label']: obj['score'] for obj in output[0]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbxleuinbMA8"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(fn=predict, inputs=\"text\", outputs=\"label\").launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
